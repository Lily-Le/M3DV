{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "kaggle_path_prefix = \"../input/mylibrary\"\n",
    "sys.path.append(kaggle_path_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from mylibrary.utils.misc import plot_voxel_enhance\n",
    "from mylibrary import augmentation\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(465, 32, 32, 32, 1)\n",
      "seg shape (465, 32, 32, 32, 1)\n",
      "(465, 2)\n",
      "(445, 2)\n"
     ]
    }
   ],
   "source": [
    "#得到训练集\n",
    "train_info=pd.read_csv(\"../input/train-data/train_val.csv\")\n",
    "train_nodule_path=\"../input/train-data/train_val/\"\n",
    "name = train_info.loc[:, 'name']\n",
    "y_train=train_info.loc[:, 'label']\n",
    "train_num=len(name)\n",
    "x_train = np.empty((train_num,*(32,32,32), 1))\n",
    "x_train_seg=np.empty((train_num,*(32,32,32), 1))\n",
    "\n",
    "for i in range(train_num):\n",
    "        with np.load(os.path.join(train_nodule_path, '%s.npz' % name[i])) as npz:\n",
    "            voxel=npz['voxel'][34:66,34:66,34:66]\n",
    "            seg=npz['seg'][34:66,34:66,34:66]\n",
    "            voxel=np.expand_dims(voxel,axis=-1)\n",
    "            seg=np.expand_dims(seg,axis=-1)\n",
    "            \n",
    "            x_train[i,]=voxel\n",
    "            x_train_seg[i,]=seg\n",
    "\n",
    "print(x_train.shape)\n",
    "print('seg shape',x_train_seg.shape)\n",
    "#np.save('x_test.npy',x_test)\n",
    "\n",
    "\n",
    "y_train=to_categorical(y_train,2)\n",
    "print(y_train.shape )\n",
    "num=train_num-20\n",
    "x_all2=x_train*x_train_seg\n",
    "y_all=y_train\n",
    "x_all_seg=x_train_seg\n",
    "xseg_train=x_all2[0:num].copy()\n",
    "yseg_train=y_train[0:num].copy()\n",
    "print(yseg_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(465, 2)\n",
      "(1800, 32, 32, 32, 1)\n",
      "(1800, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nx_train2,y_train2=argumentation.flip_dim1(x_all,y_all)\\nx_train3,y_train3=argumentation.flip_dim2(x_all,y_all)\\nx_train4,y_train4=argumentation.flip_dim3(x_all,y_all)\\n\\nx_train2,y_train2=argumentation.flip_dim2(x_train2,y_train2)\\nx_train3,y_train3=argumentation.flip_dim3(x_train3,y_train3)\\nx_train4,y_train4=argumentation.flip_dim1(x_train4,y_train4)\\narg_x3=np.concatenate((x_train2,x_train3,x_train4))\\narg_y3=np.concatenate((y_train2,y_train3,y_train4))\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#数据增强\n",
    "all_num=465\n",
    "num=all_num-20\n",
    "'''\n",
    "index1=np.random.randint(0,all_num-20,200)\n",
    "index2=np.random.randint(0,all_num-20,200)\n",
    "index3=np.random.randint(0,all_num-20,200)\n",
    "\n",
    "\n",
    "\n",
    "x_train2=x_all2[index1].copy()\n",
    "y_train2=y_all[index1].copy()\n",
    "seg2=x_all_seg[index1].copy()\n",
    "x_train3=x_all2[index2].copy()\n",
    "y_train3=y_all[index2].copy()\n",
    "seg3=x_all_seg[index2].copy()\n",
    "x_train4=x_all2[index3].copy()\n",
    "y_train4=y_all[index3].copy()\n",
    "seg4=x_all_seg[index3].copy()\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "x_train2=x_all2[0:num].copy()\n",
    "y_train2=y_all[0:num].copy()\n",
    "seg2=x_all_seg[0:num].copy()\n",
    "x_train3=x_all2[0:num].copy()\n",
    "y_train3=y_all[0:num].copy()\n",
    "seg3=x_all_seg[0:num].copy()\n",
    "x_train4=x_all2[0:num].copy()\n",
    "y_train4=y_all[0:num].copy()\n",
    "seg4=x_all_seg[0:num].copy()\n",
    "\n",
    "x_train2,y_train2,seg2=argumentation.flip_dim1(x_train2,y_train2,seg2)\n",
    "x_train3,y_train3,seg3=argumentation.flip_dim2(x_train3,y_train3,seg3)\n",
    "x_train4,y_train4,seg4=argumentation.flip_dim3(x_train4,y_train4,seg4)\n",
    "\n",
    "\n",
    "\n",
    "x_train_mix,y_train_mix,seg_mix=argumentation.mix_up(x_all2[0:num],y_all[0:num],x_all_seg[0:num],0.7,200)\n",
    "\n",
    "print(y_all.shape)\n",
    "#x_train=x_train*seg_train\n",
    "arg_x1=np.concatenate((x_train2,x_train3,x_train4,x_train_mix))\n",
    "arg_y1=np.concatenate((y_train2,y_train3,y_train4,y_train_mix))\n",
    "arg_seg=np.concatenate((seg2,seg3,seg4,seg_mix))\n",
    "\n",
    "x_train_mix2,y_train_mix2,seg_mix2=argumentation.mix_up(arg_x1,arg_y1,arg_seg,0.7,800)\n",
    "\n",
    "arg_x1=np.concatenate((x_train2,x_train3,x_train4,x_all2))\n",
    "arg_y1=np.concatenate((y_train2,y_train3,y_train4,y_all))\n",
    "\n",
    "print(arg_x1.shape)\n",
    "print(arg_y1.shape)\n",
    "#print(arg_y2[60])\n",
    "#print(arg_x2[60][15][:][15][0])\n",
    "\n",
    "\"\"\"\n",
    "x_train2,y_train2=argumentation.flip_dim1(x_all,y_all)\n",
    "x_train3,y_train3=argumentation.flip_dim2(x_all,y_all)\n",
    "x_train4,y_train4=argumentation.flip_dim3(x_all,y_all)\n",
    "\n",
    "x_train2,y_train2=argumentation.flip_dim2(x_train2,y_train2)\n",
    "x_train3,y_train3=argumentation.flip_dim3(x_train3,y_train3)\n",
    "x_train4,y_train4=argumentation.flip_dim1(x_train4,y_train4)\n",
    "arg_x3=np.concatenate((x_train2,x_train3,x_train4))\n",
    "arg_y3=np.concatenate((y_train2,y_train3,y_train4))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_voxel_enhance(arg_x1[1305].squeeze(),alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version 2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "#from keras.utils import to_categorical\n",
    "print(\"TensorFlow version\",tf.__version__)\n",
    "#print(\"Keras version\",keras.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv3D, MaxPooling3D, Dropout, BatchNormalization,MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.utils import to_categorical\n",
    "from mylibrary.models.misc import set_gpu_usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 260 samples, validate on 112 samples\n",
      "Epoch 1/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 1.3272 - accuracy: 0.5078\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.50000, saving model to Kfoldmixseg-epo-01-acc-0.50-val-0.50.hdf5\n",
      "260/260 [==============================] - 1s 6ms/sample - loss: 1.3214 - accuracy: 0.5038 - val_loss: 8.7262 - val_accuracy: 0.5000\n",
      "Epoch 2/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.9069 - accuracy: 0.6198\n",
      "Epoch 00002: val_accuracy did not improve from 0.50000\n",
      "260/260 [==============================] - 0s 898us/sample - loss: 0.9227 - accuracy: 0.6000 - val_loss: 7.8052 - val_accuracy: 0.5000\n",
      "Epoch 3/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 1.0384 - accuracy: 0.5312\n",
      "Epoch 00003: val_accuracy did not improve from 0.50000\n",
      "260/260 [==============================] - 0s 889us/sample - loss: 1.0292 - accuracy: 0.5500 - val_loss: 6.6852 - val_accuracy: 0.5000\n",
      "Epoch 4/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.9875 - accuracy: 0.5885\n",
      "Epoch 00004: val_accuracy did not improve from 0.50000\n",
      "260/260 [==============================] - 0s 913us/sample - loss: 0.9768 - accuracy: 0.5962 - val_loss: 4.6438 - val_accuracy: 0.5000\n",
      "Epoch 5/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7621 - accuracy: 0.6667\n",
      "Epoch 00005: val_accuracy did not improve from 0.50000\n",
      "260/260 [==============================] - 0s 907us/sample - loss: 0.7919 - accuracy: 0.6423 - val_loss: 3.2529 - val_accuracy: 0.5000\n",
      "Epoch 6/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8173 - accuracy: 0.5885\n",
      "Epoch 00006: val_accuracy did not improve from 0.50000\n",
      "260/260 [==============================] - 0s 906us/sample - loss: 0.8758 - accuracy: 0.5846 - val_loss: 2.2637 - val_accuracy: 0.5000\n",
      "Epoch 7/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8236 - accuracy: 0.6094\n",
      "Epoch 00007: val_accuracy improved from 0.50000 to 0.51786, saving model to Kfoldmixseg-epo-07-acc-0.60-val-0.52.hdf5\n",
      "260/260 [==============================] - 0s 1ms/sample - loss: 0.8543 - accuracy: 0.6000 - val_loss: 1.5073 - val_accuracy: 0.5179\n",
      "Epoch 8/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8325 - accuracy: 0.6146\n",
      "Epoch 00008: val_accuracy did not improve from 0.51786\n",
      "260/260 [==============================] - 0s 887us/sample - loss: 0.8646 - accuracy: 0.6115 - val_loss: 1.1857 - val_accuracy: 0.5089\n",
      "Epoch 9/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7402 - accuracy: 0.6719\n",
      "Epoch 00009: val_accuracy did not improve from 0.51786\n",
      "260/260 [==============================] - 0s 922us/sample - loss: 0.7739 - accuracy: 0.6462 - val_loss: 1.0014 - val_accuracy: 0.4911\n",
      "Epoch 10/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.9347 - accuracy: 0.5469\n",
      "Epoch 00010: val_accuracy improved from 0.51786 to 0.52679, saving model to Kfoldmixseg-epo-10-acc-0.55-val-0.53.hdf5\n",
      "260/260 [==============================] - 0s 1ms/sample - loss: 0.9186 - accuracy: 0.5500 - val_loss: 0.9636 - val_accuracy: 0.5268\n",
      "Epoch 11/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.9493 - accuracy: 0.5417\n",
      "Epoch 00011: val_accuracy did not improve from 0.52679\n",
      "260/260 [==============================] - 0s 903us/sample - loss: 0.9333 - accuracy: 0.5538 - val_loss: 0.9618 - val_accuracy: 0.5268\n",
      "Epoch 12/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.6866 - accuracy: 0.6354\n",
      "Epoch 00012: val_accuracy improved from 0.52679 to 0.55357, saving model to Kfoldmixseg-epo-12-acc-0.65-val-0.55.hdf5\n",
      "260/260 [==============================] - 0s 1ms/sample - loss: 0.6986 - accuracy: 0.6500 - val_loss: 1.0017 - val_accuracy: 0.5536\n",
      "Epoch 13/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8478 - accuracy: 0.6198\n",
      "Epoch 00013: val_accuracy did not improve from 0.55357\n",
      "260/260 [==============================] - 0s 925us/sample - loss: 0.8319 - accuracy: 0.6077 - val_loss: 1.0107 - val_accuracy: 0.5536\n",
      "Epoch 14/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.8026 - accuracy: 0.6172\n",
      "Epoch 00014: val_accuracy did not improve from 0.55357\n",
      "260/260 [==============================] - 0s 1ms/sample - loss: 0.7994 - accuracy: 0.6154 - val_loss: 0.9965 - val_accuracy: 0.5536\n",
      "Epoch 15/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8472 - accuracy: 0.6094\n",
      "Epoch 00015: val_accuracy improved from 0.55357 to 0.57143, saving model to Kfoldmixseg-epo-15-acc-0.61-val-0.57.hdf5\n",
      "260/260 [==============================] - 0s 1ms/sample - loss: 0.8610 - accuracy: 0.6077 - val_loss: 0.9028 - val_accuracy: 0.5714\n",
      "Epoch 16/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8747 - accuracy: 0.5990\n",
      "Epoch 00016: val_accuracy improved from 0.57143 to 0.58036, saving model to Kfoldmixseg-epo-16-acc-0.61-val-0.58.hdf5\n",
      "260/260 [==============================] - 0s 1ms/sample - loss: 0.8382 - accuracy: 0.6077 - val_loss: 0.8284 - val_accuracy: 0.5804\n",
      "Epoch 17/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.8552 - accuracy: 0.5469\n",
      "Epoch 00017: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 992us/sample - loss: 0.8558 - accuracy: 0.5500 - val_loss: 0.7979 - val_accuracy: 0.5536\n",
      "Epoch 18/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.8145 - accuracy: 0.5977\n",
      "Epoch 00018: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 999us/sample - loss: 0.8167 - accuracy: 0.5962 - val_loss: 0.8040 - val_accuracy: 0.5357\n",
      "Epoch 19/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.7631 - accuracy: 0.6094\n",
      "Epoch 00019: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 980us/sample - loss: 0.7702 - accuracy: 0.6038 - val_loss: 0.7958 - val_accuracy: 0.5625\n",
      "Epoch 20/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.9133 - accuracy: 0.5742\n",
      "Epoch 00020: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 958us/sample - loss: 0.9149 - accuracy: 0.5654 - val_loss: 0.8130 - val_accuracy: 0.5804\n",
      "Epoch 21/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8991 - accuracy: 0.5885\n",
      "Epoch 00021: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 908us/sample - loss: 0.8712 - accuracy: 0.5846 - val_loss: 0.8517 - val_accuracy: 0.5536\n",
      "Epoch 22/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8085 - accuracy: 0.6250\n",
      "Epoch 00022: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 932us/sample - loss: 0.7793 - accuracy: 0.6346 - val_loss: 0.8690 - val_accuracy: 0.5536\n",
      "Epoch 23/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7729 - accuracy: 0.6823\n",
      "Epoch 00023: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 920us/sample - loss: 0.7765 - accuracy: 0.6500 - val_loss: 0.8962 - val_accuracy: 0.5357\n",
      "Epoch 24/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8561 - accuracy: 0.5938\n",
      "Epoch 00024: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 959us/sample - loss: 0.7751 - accuracy: 0.6231 - val_loss: 0.9059 - val_accuracy: 0.5089\n",
      "Epoch 25/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.7359 - accuracy: 0.6484\n",
      "Epoch 00025: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 1ms/sample - loss: 0.7452 - accuracy: 0.6423 - val_loss: 0.8689 - val_accuracy: 0.5536\n",
      "Epoch 26/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.7754 - accuracy: 0.6406\n",
      "Epoch 00026: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 1ms/sample - loss: 0.7780 - accuracy: 0.6346 - val_loss: 0.8092 - val_accuracy: 0.5804\n",
      "Epoch 27/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.7917 - accuracy: 0.6016\n",
      "Epoch 00027: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 1ms/sample - loss: 0.7907 - accuracy: 0.6038 - val_loss: 0.7805 - val_accuracy: 0.5714\n",
      "Epoch 28/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.8502 - accuracy: 0.5625\n",
      "Epoch 00028: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 953us/sample - loss: 0.8467 - accuracy: 0.5654 - val_loss: 0.7794 - val_accuracy: 0.5625\n",
      "Epoch 29/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.9213 - accuracy: 0.5260\n",
      "Epoch 00029: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 902us/sample - loss: 0.8723 - accuracy: 0.5538 - val_loss: 0.7929 - val_accuracy: 0.5714\n",
      "Epoch 30/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8315 - accuracy: 0.5625\n",
      "Epoch 00030: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 927us/sample - loss: 0.8103 - accuracy: 0.5692 - val_loss: 0.8141 - val_accuracy: 0.5625\n",
      "Epoch 31/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8006 - accuracy: 0.6354\n",
      "Epoch 00031: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 926us/sample - loss: 0.7612 - accuracy: 0.6500 - val_loss: 0.8287 - val_accuracy: 0.5536\n",
      "Epoch 32/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7721 - accuracy: 0.6094\n",
      "Epoch 00032: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 919us/sample - loss: 0.7662 - accuracy: 0.6077 - val_loss: 0.8452 - val_accuracy: 0.5536\n",
      "Epoch 33/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8022 - accuracy: 0.5833\n",
      "Epoch 00033: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 908us/sample - loss: 0.7797 - accuracy: 0.5885 - val_loss: 0.8467 - val_accuracy: 0.5804\n",
      "Epoch 34/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.6611 - accuracy: 0.6667\n",
      "Epoch 00034: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 899us/sample - loss: 0.6936 - accuracy: 0.6654 - val_loss: 0.8450 - val_accuracy: 0.5625\n",
      "Epoch 35/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7441 - accuracy: 0.6094\n",
      "Epoch 00035: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 955us/sample - loss: 0.7299 - accuracy: 0.6115 - val_loss: 0.8471 - val_accuracy: 0.5536\n",
      "Epoch 36/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7022 - accuracy: 0.6562\n",
      "Epoch 00036: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 922us/sample - loss: 0.7076 - accuracy: 0.6654 - val_loss: 0.8586 - val_accuracy: 0.5625\n",
      "Epoch 37/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7526 - accuracy: 0.6354\n",
      "Epoch 00037: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 911us/sample - loss: 0.7993 - accuracy: 0.6308 - val_loss: 0.8619 - val_accuracy: 0.5714\n",
      "Epoch 38/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7651 - accuracy: 0.6354\n",
      "Epoch 00038: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 933us/sample - loss: 0.7425 - accuracy: 0.6462 - val_loss: 0.8513 - val_accuracy: 0.5714\n",
      "Epoch 39/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.7547 - accuracy: 0.6211\n",
      "Epoch 00039: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 1ms/sample - loss: 0.7606 - accuracy: 0.6192 - val_loss: 0.8526 - val_accuracy: 0.5714\n",
      "Epoch 40/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8248 - accuracy: 0.6198\n",
      "Epoch 00040: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 936us/sample - loss: 0.7791 - accuracy: 0.6385 - val_loss: 0.8551 - val_accuracy: 0.5625\n",
      "Epoch 41/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.6309 - accuracy: 0.6823\n",
      "Epoch 00041: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 915us/sample - loss: 0.6877 - accuracy: 0.6500 - val_loss: 0.8569 - val_accuracy: 0.5625\n",
      "Epoch 42/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8671 - accuracy: 0.5469\n",
      "Epoch 00042: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 930us/sample - loss: 0.8599 - accuracy: 0.5423 - val_loss: 0.8597 - val_accuracy: 0.5625\n",
      "Epoch 43/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7683 - accuracy: 0.6094\n",
      "Epoch 00043: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 918us/sample - loss: 0.7690 - accuracy: 0.5962 - val_loss: 0.8613 - val_accuracy: 0.5625\n",
      "Epoch 44/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.8044 - accuracy: 0.6016\n",
      "Epoch 00044: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 951us/sample - loss: 0.8084 - accuracy: 0.5962 - val_loss: 0.8565 - val_accuracy: 0.5714\n",
      "Epoch 45/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7954 - accuracy: 0.6146\n",
      "Epoch 00045: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 917us/sample - loss: 0.7812 - accuracy: 0.6154 - val_loss: 0.8560 - val_accuracy: 0.5625\n",
      "Epoch 46/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7494 - accuracy: 0.6458\n",
      "Epoch 00046: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 902us/sample - loss: 0.8120 - accuracy: 0.6231 - val_loss: 0.8627 - val_accuracy: 0.5625\n",
      "Epoch 47/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7966 - accuracy: 0.6198\n",
      "Epoch 00047: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 990us/sample - loss: 0.7961 - accuracy: 0.6346 - val_loss: 0.8636 - val_accuracy: 0.5714\n",
      "Epoch 48/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.7101 - accuracy: 0.6211\n",
      "Epoch 00048: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 1ms/sample - loss: 0.7089 - accuracy: 0.6231 - val_loss: 0.8636 - val_accuracy: 0.5804\n",
      "Epoch 49/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.7409 - accuracy: 0.6562\n",
      "Epoch 00049: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 1ms/sample - loss: 0.7376 - accuracy: 0.6577 - val_loss: 0.8598 - val_accuracy: 0.5714\n",
      "Epoch 50/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.7630 - accuracy: 0.5820\n",
      "Epoch 00050: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 1ms/sample - loss: 0.7610 - accuracy: 0.5808 - val_loss: 0.8629 - val_accuracy: 0.5804\n",
      "accuracy: 59.14%\n",
      "Train on 260 samples, validate on 112 samples\n",
      "Epoch 1/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 1.1743 - accuracy: 0.5000\n",
      "Epoch 00001: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 2s 7ms/sample - loss: 1.1727 - accuracy: 0.5038 - val_loss: 5.3227 - val_accuracy: 0.5000\n",
      "Epoch 2/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 1.1053 - accuracy: 0.5586\n",
      "Epoch 00002: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 1ms/sample - loss: 1.0976 - accuracy: 0.5615 - val_loss: 5.4401 - val_accuracy: 0.5179\n",
      "Epoch 3/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.7963 - accuracy: 0.6055\n",
      "Epoch 00003: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 1ms/sample - loss: 0.8099 - accuracy: 0.6038 - val_loss: 4.4679 - val_accuracy: 0.5179\n",
      "Epoch 4/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.8852 - accuracy: 0.6016\n",
      "Epoch 00004: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 1ms/sample - loss: 0.8758 - accuracy: 0.6077 - val_loss: 2.5556 - val_accuracy: 0.5000\n",
      "Epoch 5/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8827 - accuracy: 0.6354\n",
      "Epoch 00005: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 917us/sample - loss: 0.8499 - accuracy: 0.6231 - val_loss: 1.6087 - val_accuracy: 0.5000\n",
      "Epoch 6/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8270 - accuracy: 0.5573\n",
      "Epoch 00006: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 924us/sample - loss: 0.8310 - accuracy: 0.5654 - val_loss: 1.2678 - val_accuracy: 0.5536\n",
      "Epoch 7/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8358 - accuracy: 0.6094\n",
      "Epoch 00007: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 914us/sample - loss: 0.8481 - accuracy: 0.6077 - val_loss: 1.1292 - val_accuracy: 0.5357\n",
      "Epoch 8/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7000 - accuracy: 0.6771\n",
      "Epoch 00008: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 905us/sample - loss: 0.7114 - accuracy: 0.6692 - val_loss: 1.0352 - val_accuracy: 0.5000\n",
      "Epoch 9/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8667 - accuracy: 0.6250\n",
      "Epoch 00009: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 929us/sample - loss: 0.8785 - accuracy: 0.6038 - val_loss: 1.0116 - val_accuracy: 0.4911\n",
      "Epoch 10/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8552 - accuracy: 0.6094\n",
      "Epoch 00010: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 897us/sample - loss: 0.8719 - accuracy: 0.6038 - val_loss: 1.0212 - val_accuracy: 0.5179\n",
      "Epoch 11/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8730 - accuracy: 0.5729\n",
      "Epoch 00011: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 916us/sample - loss: 0.8618 - accuracy: 0.5846 - val_loss: 1.0137 - val_accuracy: 0.5446\n",
      "Epoch 12/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8661 - accuracy: 0.5938\n",
      "Epoch 00012: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 914us/sample - loss: 0.8362 - accuracy: 0.5923 - val_loss: 1.0137 - val_accuracy: 0.5536\n",
      "Epoch 13/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.7375 - accuracy: 0.6328\n",
      "Epoch 00013: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 965us/sample - loss: 0.7592 - accuracy: 0.6308 - val_loss: 1.0012 - val_accuracy: 0.5714\n",
      "Epoch 14/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8755 - accuracy: 0.6146\n",
      "Epoch 00014: val_accuracy did not improve from 0.58036\n",
      "260/260 [==============================] - 0s 915us/sample - loss: 0.8481 - accuracy: 0.6269 - val_loss: 0.8984 - val_accuracy: 0.5804\n",
      "Epoch 15/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.9239 - accuracy: 0.5885\n",
      "Epoch 00015: val_accuracy improved from 0.58036 to 0.59821, saving model to Kfoldmixseg-epo-15-acc-0.58-val-0.60.hdf5\n",
      "260/260 [==============================] - 0s 1ms/sample - loss: 0.9708 - accuracy: 0.5808 - val_loss: 0.8449 - val_accuracy: 0.5982\n",
      "Epoch 16/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7620 - accuracy: 0.6823\n",
      "Epoch 00016: val_accuracy did not improve from 0.59821\n",
      "260/260 [==============================] - 0s 906us/sample - loss: 0.7998 - accuracy: 0.6500 - val_loss: 0.8095 - val_accuracy: 0.5714\n",
      "Epoch 17/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8094 - accuracy: 0.6042\n",
      "Epoch 00017: val_accuracy did not improve from 0.59821\n",
      "260/260 [==============================] - 0s 924us/sample - loss: 0.8034 - accuracy: 0.6231 - val_loss: 0.7880 - val_accuracy: 0.5536\n",
      "Epoch 18/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8412 - accuracy: 0.6042\n",
      "Epoch 00018: val_accuracy did not improve from 0.59821\n",
      "260/260 [==============================] - 0s 912us/sample - loss: 0.8116 - accuracy: 0.6154 - val_loss: 0.7901 - val_accuracy: 0.5804\n",
      "Epoch 19/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8274 - accuracy: 0.6042\n",
      "Epoch 00019: val_accuracy did not improve from 0.59821\n",
      "260/260 [==============================] - 0s 966us/sample - loss: 0.8074 - accuracy: 0.6231 - val_loss: 0.8501 - val_accuracy: 0.5804\n",
      "Epoch 20/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8112 - accuracy: 0.6354\n",
      "Epoch 00020: val_accuracy did not improve from 0.59821\n",
      "260/260 [==============================] - 0s 967us/sample - loss: 0.8127 - accuracy: 0.6577 - val_loss: 0.8926 - val_accuracy: 0.5893\n",
      "Epoch 21/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8626 - accuracy: 0.6146\n",
      "Epoch 00021: val_accuracy did not improve from 0.59821\n",
      "260/260 [==============================] - 0s 921us/sample - loss: 0.9007 - accuracy: 0.6154 - val_loss: 0.8609 - val_accuracy: 0.5804\n",
      "Epoch 22/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7930 - accuracy: 0.6302\n",
      "Epoch 00022: val_accuracy did not improve from 0.59821\n",
      "260/260 [==============================] - 0s 922us/sample - loss: 0.7780 - accuracy: 0.6231 - val_loss: 0.8165 - val_accuracy: 0.5804\n",
      "Epoch 23/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7386 - accuracy: 0.6198\n",
      "Epoch 00023: val_accuracy did not improve from 0.59821\n",
      "260/260 [==============================] - 0s 921us/sample - loss: 0.7704 - accuracy: 0.6115 - val_loss: 0.7937 - val_accuracy: 0.5714\n",
      "Epoch 24/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7137 - accuracy: 0.6719\n",
      "Epoch 00024: val_accuracy did not improve from 0.59821\n",
      "260/260 [==============================] - 0s 927us/sample - loss: 0.6973 - accuracy: 0.6769 - val_loss: 0.8013 - val_accuracy: 0.5625\n",
      "Epoch 25/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.6683 - accuracy: 0.6823\n",
      "Epoch 00025: val_accuracy did not improve from 0.59821\n",
      "260/260 [==============================] - 0s 940us/sample - loss: 0.6814 - accuracy: 0.6731 - val_loss: 0.8110 - val_accuracy: 0.5625\n",
      "Epoch 26/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7298 - accuracy: 0.6615\n",
      "Epoch 00026: val_accuracy did not improve from 0.59821\n",
      "260/260 [==============================] - 0s 927us/sample - loss: 0.7850 - accuracy: 0.6231 - val_loss: 0.8079 - val_accuracy: 0.5893\n",
      "Epoch 27/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7246 - accuracy: 0.6354\n",
      "Epoch 00027: val_accuracy improved from 0.59821 to 0.60714, saving model to Kfoldmixseg-epo-27-acc-0.64-val-0.61.hdf5\n",
      "260/260 [==============================] - 0s 1ms/sample - loss: 0.7075 - accuracy: 0.6385 - val_loss: 0.8125 - val_accuracy: 0.6071\n",
      "Epoch 28/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7720 - accuracy: 0.6198\n",
      "Epoch 00028: val_accuracy did not improve from 0.60714\n",
      "260/260 [==============================] - 0s 894us/sample - loss: 0.7656 - accuracy: 0.6423 - val_loss: 0.8250 - val_accuracy: 0.5982\n",
      "Epoch 29/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7842 - accuracy: 0.6510\n",
      "Epoch 00029: val_accuracy did not improve from 0.60714\n",
      "260/260 [==============================] - 0s 932us/sample - loss: 0.7577 - accuracy: 0.6615 - val_loss: 0.8260 - val_accuracy: 0.5893\n",
      "Epoch 30/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7830 - accuracy: 0.6198\n",
      "Epoch 00030: val_accuracy did not improve from 0.60714\n",
      "260/260 [==============================] - 0s 919us/sample - loss: 0.7663 - accuracy: 0.6192 - val_loss: 0.8295 - val_accuracy: 0.5893\n",
      "Epoch 31/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7729 - accuracy: 0.6250\n",
      "Epoch 00031: val_accuracy did not improve from 0.60714\n",
      "260/260 [==============================] - 0s 955us/sample - loss: 0.7779 - accuracy: 0.6115 - val_loss: 0.8181 - val_accuracy: 0.6071\n",
      "Epoch 32/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7786 - accuracy: 0.6250\n",
      "Epoch 00032: val_accuracy did not improve from 0.60714\n",
      "260/260 [==============================] - 0s 905us/sample - loss: 0.7938 - accuracy: 0.6115 - val_loss: 0.8113 - val_accuracy: 0.5982\n",
      "Epoch 33/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7950 - accuracy: 0.5990\n",
      "Epoch 00033: val_accuracy did not improve from 0.60714\n",
      "260/260 [==============================] - 0s 895us/sample - loss: 0.7576 - accuracy: 0.6115 - val_loss: 0.8252 - val_accuracy: 0.5714\n",
      "Epoch 34/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.7350 - accuracy: 0.6328\n",
      "Epoch 00034: val_accuracy did not improve from 0.60714\n",
      "260/260 [==============================] - 0s 973us/sample - loss: 0.7484 - accuracy: 0.6269 - val_loss: 0.8483 - val_accuracy: 0.5536\n",
      "Epoch 35/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7001 - accuracy: 0.6979\n",
      "Epoch 00035: val_accuracy did not improve from 0.60714\n",
      "260/260 [==============================] - 0s 905us/sample - loss: 0.7120 - accuracy: 0.6885 - val_loss: 0.8363 - val_accuracy: 0.5625\n",
      "Epoch 36/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7777 - accuracy: 0.6302\n",
      "Epoch 00036: val_accuracy did not improve from 0.60714\n",
      "260/260 [==============================] - 0s 911us/sample - loss: 0.7580 - accuracy: 0.6077 - val_loss: 0.8140 - val_accuracy: 0.5714\n",
      "Epoch 37/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.7065 - accuracy: 0.6719\n",
      "Epoch 00037: val_accuracy did not improve from 0.60714\n",
      "260/260 [==============================] - 0s 1ms/sample - loss: 0.7010 - accuracy: 0.6769 - val_loss: 0.8036 - val_accuracy: 0.5804\n",
      "Epoch 38/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7401 - accuracy: 0.6562\n",
      "Epoch 00038: val_accuracy did not improve from 0.60714\n",
      "260/260 [==============================] - 0s 922us/sample - loss: 0.7373 - accuracy: 0.6654 - val_loss: 0.8004 - val_accuracy: 0.5714\n",
      "Epoch 39/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7569 - accuracy: 0.6250\n",
      "Epoch 00039: val_accuracy did not improve from 0.60714\n",
      "260/260 [==============================] - 0s 906us/sample - loss: 0.7747 - accuracy: 0.6269 - val_loss: 0.8017 - val_accuracy: 0.5804\n",
      "Epoch 40/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7311 - accuracy: 0.6771\n",
      "Epoch 00040: val_accuracy did not improve from 0.60714\n",
      "260/260 [==============================] - 0s 916us/sample - loss: 0.7320 - accuracy: 0.6692 - val_loss: 0.7980 - val_accuracy: 0.6071\n",
      "Epoch 41/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7241 - accuracy: 0.6458\n",
      "Epoch 00041: val_accuracy did not improve from 0.60714\n",
      "260/260 [==============================] - 0s 920us/sample - loss: 0.7305 - accuracy: 0.6462 - val_loss: 0.7942 - val_accuracy: 0.5982\n",
      "Epoch 42/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.6548 - accuracy: 0.6510\n",
      "Epoch 00042: val_accuracy did not improve from 0.60714\n",
      "260/260 [==============================] - 0s 929us/sample - loss: 0.6682 - accuracy: 0.6654 - val_loss: 0.7844 - val_accuracy: 0.5804\n",
      "Epoch 43/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.9060 - accuracy: 0.5573\n",
      "Epoch 00043: val_accuracy did not improve from 0.60714\n",
      "260/260 [==============================] - 0s 994us/sample - loss: 0.8499 - accuracy: 0.5885 - val_loss: 0.7731 - val_accuracy: 0.5893\n",
      "Epoch 44/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7631 - accuracy: 0.6823\n",
      "Epoch 00044: val_accuracy did not improve from 0.60714\n",
      "260/260 [==============================] - 0s 921us/sample - loss: 0.7906 - accuracy: 0.6577 - val_loss: 0.7656 - val_accuracy: 0.5804\n",
      "Epoch 45/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.6575 - accuracy: 0.6719\n",
      "Epoch 00045: val_accuracy did not improve from 0.60714\n",
      "260/260 [==============================] - 0s 910us/sample - loss: 0.6785 - accuracy: 0.6692 - val_loss: 0.7630 - val_accuracy: 0.5804\n",
      "Epoch 46/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.6105 - accuracy: 0.6979\n",
      "Epoch 00046: val_accuracy did not improve from 0.60714\n",
      "260/260 [==============================] - 0s 883us/sample - loss: 0.6667 - accuracy: 0.6615 - val_loss: 0.7480 - val_accuracy: 0.5804\n",
      "Epoch 47/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.6812 - accuracy: 0.6523\n",
      "Epoch 00047: val_accuracy did not improve from 0.60714\n",
      "260/260 [==============================] - 0s 981us/sample - loss: 0.6793 - accuracy: 0.6500 - val_loss: 0.7377 - val_accuracy: 0.5804\n",
      "Epoch 48/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.7733 - accuracy: 0.6328\n",
      "Epoch 00048: val_accuracy did not improve from 0.60714\n",
      "260/260 [==============================] - 0s 1ms/sample - loss: 0.7695 - accuracy: 0.6346 - val_loss: 0.7405 - val_accuracy: 0.5714\n",
      "Epoch 49/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7880 - accuracy: 0.6094\n",
      "Epoch 00049: val_accuracy did not improve from 0.60714\n",
      "260/260 [==============================] - 0s 901us/sample - loss: 0.7491 - accuracy: 0.6231 - val_loss: 0.7567 - val_accuracy: 0.5625\n",
      "Epoch 50/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.6519 - accuracy: 0.6771\n",
      "Epoch 00050: val_accuracy did not improve from 0.60714\n",
      "260/260 [==============================] - 0s 882us/sample - loss: 0.6279 - accuracy: 0.6962 - val_loss: 0.7621 - val_accuracy: 0.5536\n",
      "accuracy: 61.29%\n",
      "Train on 260 samples, validate on 112 samples\n",
      "Epoch 1/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.9587 - accuracy: 0.5664\n",
      "Epoch 00001: val_accuracy did not improve from 0.60714\n",
      "260/260 [==============================] - 1s 5ms/sample - loss: 0.9508 - accuracy: 0.5731 - val_loss: 4.3491 - val_accuracy: 0.4107\n",
      "Epoch 2/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 1.0347 - accuracy: 0.5677\n",
      "Epoch 00002: val_accuracy did not improve from 0.60714\n",
      "260/260 [==============================] - 0s 925us/sample - loss: 1.0356 - accuracy: 0.5577 - val_loss: 2.6561 - val_accuracy: 0.4375\n",
      "Epoch 3/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 1.0200 - accuracy: 0.4896\n",
      "Epoch 00003: val_accuracy did not improve from 0.60714\n",
      "260/260 [==============================] - 0s 941us/sample - loss: 0.9872 - accuracy: 0.4923 - val_loss: 2.7828 - val_accuracy: 0.4018\n",
      "Epoch 4/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 1.0070 - accuracy: 0.5208\n",
      "Epoch 00004: val_accuracy did not improve from 0.60714\n",
      "260/260 [==============================] - 0s 917us/sample - loss: 0.9792 - accuracy: 0.5308 - val_loss: 2.5849 - val_accuracy: 0.3839\n",
      "Epoch 5/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.9333 - accuracy: 0.5625\n",
      "Epoch 00005: val_accuracy did not improve from 0.60714\n",
      "260/260 [==============================] - 0s 984us/sample - loss: 0.9430 - accuracy: 0.5615 - val_loss: 2.1831 - val_accuracy: 0.4018\n",
      "Epoch 6/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.9675 - accuracy: 0.5625\n",
      "Epoch 00006: val_accuracy did not improve from 0.60714\n",
      "260/260 [==============================] - 0s 905us/sample - loss: 1.0068 - accuracy: 0.5385 - val_loss: 2.1090 - val_accuracy: 0.3839\n",
      "Epoch 7/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.9622 - accuracy: 0.5104\n",
      "Epoch 00007: val_accuracy did not improve from 0.60714\n",
      "260/260 [==============================] - 0s 902us/sample - loss: 0.9492 - accuracy: 0.5500 - val_loss: 2.0706 - val_accuracy: 0.3750\n",
      "Epoch 8/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8815 - accuracy: 0.5365\n",
      "Epoch 00008: val_accuracy did not improve from 0.60714\n",
      "260/260 [==============================] - 0s 912us/sample - loss: 0.9061 - accuracy: 0.5346 - val_loss: 1.8673 - val_accuracy: 0.4018\n",
      "Epoch 9/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.9272 - accuracy: 0.5573\n",
      "Epoch 00009: val_accuracy did not improve from 0.60714\n",
      "260/260 [==============================] - 0s 919us/sample - loss: 0.9167 - accuracy: 0.5615 - val_loss: 1.5775 - val_accuracy: 0.4286\n",
      "Epoch 10/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8716 - accuracy: 0.5365\n",
      "Epoch 00010: val_accuracy did not improve from 0.60714\n",
      "260/260 [==============================] - 0s 892us/sample - loss: 0.8419 - accuracy: 0.5462 - val_loss: 1.3203 - val_accuracy: 0.4821\n",
      "Epoch 11/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.8470 - accuracy: 0.5547\n",
      "Epoch 00011: val_accuracy did not improve from 0.60714\n",
      "260/260 [==============================] - 0s 955us/sample - loss: 0.8400 - accuracy: 0.5577 - val_loss: 1.0522 - val_accuracy: 0.5089\n",
      "Epoch 12/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7823 - accuracy: 0.5990\n",
      "Epoch 00012: val_accuracy did not improve from 0.60714\n",
      "260/260 [==============================] - 0s 907us/sample - loss: 0.7854 - accuracy: 0.5846 - val_loss: 0.8223 - val_accuracy: 0.5714\n",
      "Epoch 13/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8565 - accuracy: 0.5938\n",
      "Epoch 00013: val_accuracy did not improve from 0.60714\n",
      "260/260 [==============================] - 0s 940us/sample - loss: 0.8695 - accuracy: 0.5769 - val_loss: 0.7425 - val_accuracy: 0.5893\n",
      "Epoch 14/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8464 - accuracy: 0.5729\n",
      "Epoch 00014: val_accuracy improved from 0.60714 to 0.62500, saving model to Kfoldmixseg-epo-14-acc-0.60-val-0.62.hdf5\n",
      "260/260 [==============================] - 0s 1ms/sample - loss: 0.8346 - accuracy: 0.5962 - val_loss: 0.7164 - val_accuracy: 0.6250\n",
      "Epoch 15/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7866 - accuracy: 0.5469\n",
      "Epoch 00015: val_accuracy did not improve from 0.62500\n",
      "260/260 [==============================] - 0s 932us/sample - loss: 0.7821 - accuracy: 0.5654 - val_loss: 0.7172 - val_accuracy: 0.5893\n",
      "Epoch 16/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.9474 - accuracy: 0.5469\n",
      "Epoch 00016: val_accuracy did not improve from 0.62500\n",
      "260/260 [==============================] - 0s 912us/sample - loss: 0.8782 - accuracy: 0.5615 - val_loss: 0.7032 - val_accuracy: 0.5982\n",
      "Epoch 17/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8066 - accuracy: 0.5573\n",
      "Epoch 00017: val_accuracy improved from 0.62500 to 0.63393, saving model to Kfoldmixseg-epo-17-acc-0.55-val-0.63.hdf5\n",
      "260/260 [==============================] - 0s 1ms/sample - loss: 0.8778 - accuracy: 0.5462 - val_loss: 0.6832 - val_accuracy: 0.6339\n",
      "Epoch 18/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8221 - accuracy: 0.6250\n",
      "Epoch 00018: val_accuracy did not improve from 0.63393\n",
      "260/260 [==============================] - 0s 914us/sample - loss: 0.8396 - accuracy: 0.6192 - val_loss: 0.6736 - val_accuracy: 0.6339\n",
      "Epoch 19/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7524 - accuracy: 0.6719\n",
      "Epoch 00019: val_accuracy improved from 0.63393 to 0.64286, saving model to Kfoldmixseg-epo-19-acc-0.65-val-0.64.hdf5\n",
      "260/260 [==============================] - 0s 1ms/sample - loss: 0.7891 - accuracy: 0.6462 - val_loss: 0.6773 - val_accuracy: 0.6429\n",
      "Epoch 20/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8506 - accuracy: 0.5885\n",
      "Epoch 00020: val_accuracy did not improve from 0.64286\n",
      "260/260 [==============================] - 0s 917us/sample - loss: 0.8062 - accuracy: 0.6115 - val_loss: 0.6598 - val_accuracy: 0.6429\n",
      "Epoch 21/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8261 - accuracy: 0.5677\n",
      "Epoch 00021: val_accuracy improved from 0.64286 to 0.65179, saving model to Kfoldmixseg-epo-21-acc-0.58-val-0.65.hdf5\n",
      "260/260 [==============================] - 0s 1ms/sample - loss: 0.8096 - accuracy: 0.5808 - val_loss: 0.6435 - val_accuracy: 0.6518\n",
      "Epoch 22/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7298 - accuracy: 0.6146\n",
      "Epoch 00022: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 898us/sample - loss: 0.7420 - accuracy: 0.6192 - val_loss: 0.6490 - val_accuracy: 0.6429\n",
      "Epoch 23/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8743 - accuracy: 0.5833\n",
      "Epoch 00023: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 925us/sample - loss: 0.8548 - accuracy: 0.5808 - val_loss: 0.6609 - val_accuracy: 0.6250\n",
      "Epoch 24/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.8178 - accuracy: 0.5820\n",
      "Epoch 00024: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 952us/sample - loss: 0.8201 - accuracy: 0.5769 - val_loss: 0.6756 - val_accuracy: 0.6250\n",
      "Epoch 25/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7903 - accuracy: 0.5938\n",
      "Epoch 00025: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 920us/sample - loss: 0.7525 - accuracy: 0.6192 - val_loss: 0.6912 - val_accuracy: 0.6161\n",
      "Epoch 26/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7588 - accuracy: 0.6354\n",
      "Epoch 00026: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 926us/sample - loss: 0.7521 - accuracy: 0.6346 - val_loss: 0.7042 - val_accuracy: 0.6071\n",
      "Epoch 27/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7694 - accuracy: 0.5885\n",
      "Epoch 00027: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 916us/sample - loss: 0.7874 - accuracy: 0.5769 - val_loss: 0.7202 - val_accuracy: 0.6071\n",
      "Epoch 28/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8192 - accuracy: 0.5885\n",
      "Epoch 00028: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 918us/sample - loss: 0.8133 - accuracy: 0.5923 - val_loss: 0.7136 - val_accuracy: 0.6071\n",
      "Epoch 29/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7332 - accuracy: 0.6302\n",
      "Epoch 00029: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 928us/sample - loss: 0.7688 - accuracy: 0.6269 - val_loss: 0.7134 - val_accuracy: 0.6071\n",
      "Epoch 30/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7713 - accuracy: 0.5885\n",
      "Epoch 00030: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 967us/sample - loss: 0.7648 - accuracy: 0.6000 - val_loss: 0.7199 - val_accuracy: 0.6161\n",
      "Epoch 31/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.8301 - accuracy: 0.5938\n",
      "Epoch 00031: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 975us/sample - loss: 0.8326 - accuracy: 0.5923 - val_loss: 0.7317 - val_accuracy: 0.6071\n",
      "Epoch 32/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7123 - accuracy: 0.6302\n",
      "Epoch 00032: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 916us/sample - loss: 0.7245 - accuracy: 0.6308 - val_loss: 0.7502 - val_accuracy: 0.6071\n",
      "Epoch 33/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8230 - accuracy: 0.5573\n",
      "Epoch 00033: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 923us/sample - loss: 0.8112 - accuracy: 0.5692 - val_loss: 0.7912 - val_accuracy: 0.5893\n",
      "Epoch 34/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.7970 - accuracy: 0.6289\n",
      "Epoch 00034: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 1ms/sample - loss: 0.7985 - accuracy: 0.6308 - val_loss: 0.8337 - val_accuracy: 0.5893\n",
      "Epoch 35/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.7114 - accuracy: 0.6289\n",
      "Epoch 00035: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 974us/sample - loss: 0.7088 - accuracy: 0.6308 - val_loss: 0.8401 - val_accuracy: 0.5893\n",
      "Epoch 36/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7771 - accuracy: 0.6042\n",
      "Epoch 00036: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 947us/sample - loss: 0.7734 - accuracy: 0.6154 - val_loss: 0.8283 - val_accuracy: 0.5893\n",
      "Epoch 37/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.7395 - accuracy: 0.6289\n",
      "Epoch 00037: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 942us/sample - loss: 0.7432 - accuracy: 0.6308 - val_loss: 0.8161 - val_accuracy: 0.5893\n",
      "Epoch 38/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8711 - accuracy: 0.6198\n",
      "Epoch 00038: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 919us/sample - loss: 0.8265 - accuracy: 0.6385 - val_loss: 0.7933 - val_accuracy: 0.5893\n",
      "Epoch 39/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7341 - accuracy: 0.6771\n",
      "Epoch 00039: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 910us/sample - loss: 0.7383 - accuracy: 0.6538 - val_loss: 0.7890 - val_accuracy: 0.5893\n",
      "Epoch 40/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7566 - accuracy: 0.6250\n",
      "Epoch 00040: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 910us/sample - loss: 0.7454 - accuracy: 0.6269 - val_loss: 0.8126 - val_accuracy: 0.5893\n",
      "Epoch 41/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7340 - accuracy: 0.6302\n",
      "Epoch 00041: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 916us/sample - loss: 0.7608 - accuracy: 0.6192 - val_loss: 0.8179 - val_accuracy: 0.5893\n",
      "Epoch 42/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7016 - accuracy: 0.6615\n",
      "Epoch 00042: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 913us/sample - loss: 0.7103 - accuracy: 0.6577 - val_loss: 0.8122 - val_accuracy: 0.5893\n",
      "Epoch 43/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.7748 - accuracy: 0.6094\n",
      "Epoch 00043: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 951us/sample - loss: 0.7752 - accuracy: 0.6077 - val_loss: 0.8418 - val_accuracy: 0.5804\n",
      "Epoch 44/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.6775 - accuracy: 0.6302\n",
      "Epoch 00044: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 914us/sample - loss: 0.6705 - accuracy: 0.6231 - val_loss: 0.8841 - val_accuracy: 0.5804\n",
      "Epoch 45/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8413 - accuracy: 0.6042\n",
      "Epoch 00045: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 910us/sample - loss: 0.8524 - accuracy: 0.5962 - val_loss: 0.8940 - val_accuracy: 0.5893\n",
      "Epoch 46/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.6897 - accuracy: 0.6146\n",
      "Epoch 00046: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 904us/sample - loss: 0.6554 - accuracy: 0.6500 - val_loss: 0.8276 - val_accuracy: 0.5804\n",
      "Epoch 47/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8155 - accuracy: 0.6094\n",
      "Epoch 00047: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 929us/sample - loss: 0.8195 - accuracy: 0.5885 - val_loss: 0.7901 - val_accuracy: 0.5893\n",
      "Epoch 48/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.6434 - accuracy: 0.6719\n",
      "Epoch 00048: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 924us/sample - loss: 0.6662 - accuracy: 0.6654 - val_loss: 0.7422 - val_accuracy: 0.5804\n",
      "Epoch 49/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7666 - accuracy: 0.6302\n",
      "Epoch 00049: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 928us/sample - loss: 0.8285 - accuracy: 0.5962 - val_loss: 0.7130 - val_accuracy: 0.5982\n",
      "Epoch 50/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.6777 - accuracy: 0.6667\n",
      "Epoch 00050: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 881us/sample - loss: 0.7289 - accuracy: 0.6231 - val_loss: 0.7113 - val_accuracy: 0.5982\n",
      "accuracy: 63.44%\n",
      "Train on 260 samples, validate on 112 samples\n",
      "Epoch 1/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 1.2683 - accuracy: 0.5469\n",
      "Epoch 00001: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 1s 5ms/sample - loss: 1.2724 - accuracy: 0.5462 - val_loss: 7.0009 - val_accuracy: 0.5179\n",
      "Epoch 2/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 1.0676 - accuracy: 0.5052\n",
      "Epoch 00002: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 904us/sample - loss: 1.0333 - accuracy: 0.5192 - val_loss: 1.9815 - val_accuracy: 0.5000\n",
      "Epoch 3/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8986 - accuracy: 0.5885\n",
      "Epoch 00003: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 928us/sample - loss: 1.0024 - accuracy: 0.5692 - val_loss: 2.1160 - val_accuracy: 0.5089\n",
      "Epoch 4/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.9882 - accuracy: 0.5625\n",
      "Epoch 00004: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 891us/sample - loss: 0.9610 - accuracy: 0.5500 - val_loss: 1.4530 - val_accuracy: 0.4821\n",
      "Epoch 5/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 1.0013 - accuracy: 0.5990\n",
      "Epoch 00005: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 906us/sample - loss: 0.9473 - accuracy: 0.5885 - val_loss: 1.5582 - val_accuracy: 0.4911\n",
      "Epoch 6/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8692 - accuracy: 0.6510\n",
      "Epoch 00006: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 915us/sample - loss: 0.9275 - accuracy: 0.6385 - val_loss: 1.3855 - val_accuracy: 0.4732\n",
      "Epoch 7/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.9730 - accuracy: 0.5312\n",
      "Epoch 00007: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 904us/sample - loss: 0.9416 - accuracy: 0.5500 - val_loss: 1.1696 - val_accuracy: 0.4554\n",
      "Epoch 8/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8856 - accuracy: 0.5729\n",
      "Epoch 00008: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 910us/sample - loss: 0.8359 - accuracy: 0.5885 - val_loss: 1.1328 - val_accuracy: 0.4554\n",
      "Epoch 9/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.9107 - accuracy: 0.5729\n",
      "Epoch 00009: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 925us/sample - loss: 0.8760 - accuracy: 0.5846 - val_loss: 1.0155 - val_accuracy: 0.4732\n",
      "Epoch 10/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8860 - accuracy: 0.6198\n",
      "Epoch 00010: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 899us/sample - loss: 0.8918 - accuracy: 0.6154 - val_loss: 0.9437 - val_accuracy: 0.4911\n",
      "Epoch 11/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.9909 - accuracy: 0.5625\n",
      "Epoch 00011: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 949us/sample - loss: 0.9843 - accuracy: 0.5654 - val_loss: 0.8899 - val_accuracy: 0.5268\n",
      "Epoch 12/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.9236 - accuracy: 0.5833\n",
      "Epoch 00012: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 935us/sample - loss: 0.9566 - accuracy: 0.5846 - val_loss: 0.8770 - val_accuracy: 0.5089\n",
      "Epoch 13/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8621 - accuracy: 0.6042\n",
      "Epoch 00013: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 924us/sample - loss: 0.8921 - accuracy: 0.5885 - val_loss: 0.8737 - val_accuracy: 0.5000\n",
      "Epoch 14/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.9421 - accuracy: 0.5885\n",
      "Epoch 00014: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 922us/sample - loss: 0.8757 - accuracy: 0.6154 - val_loss: 0.8691 - val_accuracy: 0.5089\n",
      "Epoch 15/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.9267 - accuracy: 0.5885\n",
      "Epoch 00015: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 923us/sample - loss: 0.9639 - accuracy: 0.5808 - val_loss: 0.8438 - val_accuracy: 0.5179\n",
      "Epoch 16/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7707 - accuracy: 0.6458\n",
      "Epoch 00016: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 930us/sample - loss: 0.8602 - accuracy: 0.6038 - val_loss: 0.8070 - val_accuracy: 0.5179\n",
      "Epoch 17/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.9079 - accuracy: 0.5990\n",
      "Epoch 00017: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 930us/sample - loss: 0.8656 - accuracy: 0.5962 - val_loss: 0.7818 - val_accuracy: 0.5804\n",
      "Epoch 18/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8497 - accuracy: 0.5677\n",
      "Epoch 00018: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 932us/sample - loss: 0.8150 - accuracy: 0.5923 - val_loss: 0.7746 - val_accuracy: 0.5536\n",
      "Epoch 19/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8063 - accuracy: 0.5990\n",
      "Epoch 00019: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 972us/sample - loss: 0.7704 - accuracy: 0.6269 - val_loss: 0.7736 - val_accuracy: 0.5804\n",
      "Epoch 20/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7731 - accuracy: 0.6354\n",
      "Epoch 00020: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 889us/sample - loss: 0.7807 - accuracy: 0.6423 - val_loss: 0.7676 - val_accuracy: 0.5714\n",
      "Epoch 21/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7302 - accuracy: 0.6354\n",
      "Epoch 00021: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 906us/sample - loss: 0.7730 - accuracy: 0.6192 - val_loss: 0.7696 - val_accuracy: 0.5625\n",
      "Epoch 22/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.9260 - accuracy: 0.5469\n",
      "Epoch 00022: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 984us/sample - loss: 0.8546 - accuracy: 0.5692 - val_loss: 0.7750 - val_accuracy: 0.5804\n",
      "Epoch 23/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.8263 - accuracy: 0.6094\n",
      "Epoch 00023: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 985us/sample - loss: 0.8269 - accuracy: 0.6077 - val_loss: 0.8079 - val_accuracy: 0.5804\n",
      "Epoch 24/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8015 - accuracy: 0.5729\n",
      "Epoch 00024: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 943us/sample - loss: 0.8449 - accuracy: 0.5692 - val_loss: 0.8284 - val_accuracy: 0.5625\n",
      "Epoch 25/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.8671 - accuracy: 0.6094\n",
      "Epoch 00025: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 972us/sample - loss: 0.8618 - accuracy: 0.6077 - val_loss: 0.8372 - val_accuracy: 0.5625\n",
      "Epoch 26/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.6914 - accuracy: 0.6458\n",
      "Epoch 00026: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 937us/sample - loss: 0.7215 - accuracy: 0.6423 - val_loss: 0.8116 - val_accuracy: 0.5714\n",
      "Epoch 27/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.8266 - accuracy: 0.6172\n",
      "Epoch 00027: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 966us/sample - loss: 0.8373 - accuracy: 0.6154 - val_loss: 0.7628 - val_accuracy: 0.5893\n",
      "Epoch 28/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.6871 - accuracy: 0.6250\n",
      "Epoch 00028: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 919us/sample - loss: 0.7278 - accuracy: 0.6308 - val_loss: 0.7400 - val_accuracy: 0.5893\n",
      "Epoch 29/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7843 - accuracy: 0.6302\n",
      "Epoch 00029: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 903us/sample - loss: 0.7816 - accuracy: 0.6115 - val_loss: 0.7518 - val_accuracy: 0.5893\n",
      "Epoch 30/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7320 - accuracy: 0.5885\n",
      "Epoch 00030: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 926us/sample - loss: 0.6769 - accuracy: 0.6385 - val_loss: 0.7744 - val_accuracy: 0.5625\n",
      "Epoch 31/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8797 - accuracy: 0.5469\n",
      "Epoch 00031: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 910us/sample - loss: 0.8178 - accuracy: 0.6000 - val_loss: 0.7834 - val_accuracy: 0.5446\n",
      "Epoch 32/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7702 - accuracy: 0.6615\n",
      "Epoch 00032: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 921us/sample - loss: 0.7630 - accuracy: 0.6615 - val_loss: 0.7943 - val_accuracy: 0.5357\n",
      "Epoch 33/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7781 - accuracy: 0.6146\n",
      "Epoch 00033: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 923us/sample - loss: 0.7406 - accuracy: 0.6269 - val_loss: 0.7928 - val_accuracy: 0.5446\n",
      "Epoch 34/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7262 - accuracy: 0.6667\n",
      "Epoch 00034: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 887us/sample - loss: 0.7345 - accuracy: 0.6692 - val_loss: 0.7822 - val_accuracy: 0.5714\n",
      "Epoch 35/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7975 - accuracy: 0.6146\n",
      "Epoch 00035: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 903us/sample - loss: 0.7720 - accuracy: 0.6154 - val_loss: 0.7937 - val_accuracy: 0.5714\n",
      "Epoch 36/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.6647 - accuracy: 0.6615\n",
      "Epoch 00036: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 927us/sample - loss: 0.6968 - accuracy: 0.6423 - val_loss: 0.8378 - val_accuracy: 0.5536\n",
      "Epoch 37/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7550 - accuracy: 0.5938\n",
      "Epoch 00037: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 909us/sample - loss: 0.7717 - accuracy: 0.6038 - val_loss: 0.8573 - val_accuracy: 0.5268\n",
      "Epoch 38/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7629 - accuracy: 0.6302\n",
      "Epoch 00038: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 900us/sample - loss: 0.7480 - accuracy: 0.6346 - val_loss: 0.8409 - val_accuracy: 0.5446\n",
      "Epoch 39/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7062 - accuracy: 0.6354\n",
      "Epoch 00039: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 914us/sample - loss: 0.6672 - accuracy: 0.6500 - val_loss: 0.8607 - val_accuracy: 0.5446\n",
      "Epoch 40/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.7145 - accuracy: 0.6758\n",
      "Epoch 00040: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 929us/sample - loss: 0.7134 - accuracy: 0.6731 - val_loss: 0.8697 - val_accuracy: 0.5268\n",
      "Epoch 41/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7766 - accuracy: 0.6562\n",
      "Epoch 00041: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 906us/sample - loss: 0.7276 - accuracy: 0.6500 - val_loss: 0.9026 - val_accuracy: 0.5536\n",
      "Epoch 42/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7843 - accuracy: 0.6354\n",
      "Epoch 00042: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 930us/sample - loss: 0.7847 - accuracy: 0.6538 - val_loss: 0.8935 - val_accuracy: 0.5625\n",
      "Epoch 43/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7398 - accuracy: 0.6719\n",
      "Epoch 00043: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 915us/sample - loss: 0.7443 - accuracy: 0.6654 - val_loss: 0.8636 - val_accuracy: 0.5536\n",
      "Epoch 44/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.6971 - accuracy: 0.6302\n",
      "Epoch 00044: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 915us/sample - loss: 0.7053 - accuracy: 0.6231 - val_loss: 0.8292 - val_accuracy: 0.5625\n",
      "Epoch 45/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.6964 - accuracy: 0.6510\n",
      "Epoch 00045: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 898us/sample - loss: 0.7599 - accuracy: 0.6231 - val_loss: 0.8359 - val_accuracy: 0.5446\n",
      "Epoch 46/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.6811 - accuracy: 0.6562\n",
      "Epoch 00046: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 914us/sample - loss: 0.6497 - accuracy: 0.6615 - val_loss: 0.8790 - val_accuracy: 0.5536\n",
      "Epoch 47/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.6852 - accuracy: 0.6771\n",
      "Epoch 00047: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 916us/sample - loss: 0.6390 - accuracy: 0.7000 - val_loss: 0.9100 - val_accuracy: 0.5625\n",
      "Epoch 48/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.6835 - accuracy: 0.6458\n",
      "Epoch 00048: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 931us/sample - loss: 0.7143 - accuracy: 0.6231 - val_loss: 0.8888 - val_accuracy: 0.5625\n",
      "Epoch 49/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.6810 - accuracy: 0.6771\n",
      "Epoch 00049: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 911us/sample - loss: 0.6705 - accuracy: 0.6885 - val_loss: 0.8778 - val_accuracy: 0.5625\n",
      "Epoch 50/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.6923 - accuracy: 0.6458\n",
      "Epoch 00050: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 924us/sample - loss: 0.6789 - accuracy: 0.6654 - val_loss: 0.8928 - val_accuracy: 0.5625\n",
      "accuracy: 69.89%\n",
      "Train on 260 samples, validate on 112 samples\n",
      "Epoch 1/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 1.7098 - accuracy: 0.5195\n",
      "Epoch 00001: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 1s 6ms/sample - loss: 1.6878 - accuracy: 0.5231 - val_loss: 14.0827 - val_accuracy: 0.5089\n",
      "Epoch 2/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 1.1894 - accuracy: 0.5195\n",
      "Epoch 00002: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 987us/sample - loss: 1.1753 - accuracy: 0.5269 - val_loss: 3.8416 - val_accuracy: 0.5268\n",
      "Epoch 3/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 1.0434 - accuracy: 0.5990\n",
      "Epoch 00003: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 930us/sample - loss: 1.0265 - accuracy: 0.5923 - val_loss: 1.0481 - val_accuracy: 0.5714\n",
      "Epoch 4/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 1.0770 - accuracy: 0.5469\n",
      "Epoch 00004: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 1ms/sample - loss: 1.0898 - accuracy: 0.5423 - val_loss: 1.3404 - val_accuracy: 0.5268\n",
      "Epoch 5/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.9319 - accuracy: 0.5990\n",
      "Epoch 00005: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 932us/sample - loss: 0.9661 - accuracy: 0.5731 - val_loss: 1.1120 - val_accuracy: 0.5268\n",
      "Epoch 6/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8553 - accuracy: 0.5781\n",
      "Epoch 00006: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 970us/sample - loss: 0.8137 - accuracy: 0.5846 - val_loss: 1.0227 - val_accuracy: 0.5000\n",
      "Epoch 7/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 1.0141 - accuracy: 0.5104\n",
      "Epoch 00007: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 1ms/sample - loss: 1.0027 - accuracy: 0.5192 - val_loss: 0.9034 - val_accuracy: 0.5357\n",
      "Epoch 8/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8595 - accuracy: 0.6042\n",
      "Epoch 00008: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 933us/sample - loss: 0.8218 - accuracy: 0.6269 - val_loss: 0.9121 - val_accuracy: 0.5625\n",
      "Epoch 9/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.9342 - accuracy: 0.5664\n",
      "Epoch 00009: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 990us/sample - loss: 0.9320 - accuracy: 0.5692 - val_loss: 0.9132 - val_accuracy: 0.5446\n",
      "Epoch 10/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.8620 - accuracy: 0.6250\n",
      "Epoch 00010: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 1ms/sample - loss: 0.8658 - accuracy: 0.6231 - val_loss: 0.8732 - val_accuracy: 0.5536\n",
      "Epoch 11/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8074 - accuracy: 0.6094\n",
      "Epoch 00011: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 1ms/sample - loss: 0.8511 - accuracy: 0.6038 - val_loss: 0.8153 - val_accuracy: 0.5446\n",
      "Epoch 12/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.8129 - accuracy: 0.6445\n",
      "Epoch 00012: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 998us/sample - loss: 0.8149 - accuracy: 0.6423 - val_loss: 0.7901 - val_accuracy: 0.5536\n",
      "Epoch 13/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8437 - accuracy: 0.5833\n",
      "Epoch 00013: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 913us/sample - loss: 0.8363 - accuracy: 0.5923 - val_loss: 0.8469 - val_accuracy: 0.5179\n",
      "Epoch 14/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8265 - accuracy: 0.5990\n",
      "Epoch 00014: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 901us/sample - loss: 0.8597 - accuracy: 0.6077 - val_loss: 0.8706 - val_accuracy: 0.5268\n",
      "Epoch 15/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8078 - accuracy: 0.5938\n",
      "Epoch 00015: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 941us/sample - loss: 0.8058 - accuracy: 0.6038 - val_loss: 0.8505 - val_accuracy: 0.5357\n",
      "Epoch 16/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8308 - accuracy: 0.5885\n",
      "Epoch 00016: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 947us/sample - loss: 0.8061 - accuracy: 0.6000 - val_loss: 0.7946 - val_accuracy: 0.5268\n",
      "Epoch 17/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8672 - accuracy: 0.5885\n",
      "Epoch 00017: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 923us/sample - loss: 0.9010 - accuracy: 0.5808 - val_loss: 0.7525 - val_accuracy: 0.5804\n",
      "Epoch 18/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8212 - accuracy: 0.6250\n",
      "Epoch 00018: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 915us/sample - loss: 0.8384 - accuracy: 0.6154 - val_loss: 0.7346 - val_accuracy: 0.5714\n",
      "Epoch 19/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8231 - accuracy: 0.6146\n",
      "Epoch 00019: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 940us/sample - loss: 0.8409 - accuracy: 0.6077 - val_loss: 0.7348 - val_accuracy: 0.5804\n",
      "Epoch 20/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.7847 - accuracy: 0.6055\n",
      "Epoch 00020: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 953us/sample - loss: 0.7833 - accuracy: 0.6038 - val_loss: 0.7335 - val_accuracy: 0.5804\n",
      "Epoch 21/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.7866 - accuracy: 0.6172\n",
      "Epoch 00021: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 943us/sample - loss: 0.7907 - accuracy: 0.6115 - val_loss: 0.7411 - val_accuracy: 0.5714\n",
      "Epoch 22/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.6399 - accuracy: 0.6823\n",
      "Epoch 00022: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 919us/sample - loss: 0.6785 - accuracy: 0.6692 - val_loss: 0.7615 - val_accuracy: 0.5625\n",
      "Epoch 23/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7943 - accuracy: 0.6354\n",
      "Epoch 00023: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 924us/sample - loss: 0.8289 - accuracy: 0.6077 - val_loss: 0.7691 - val_accuracy: 0.5625\n",
      "Epoch 24/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7612 - accuracy: 0.6094\n",
      "Epoch 00024: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 919us/sample - loss: 0.7653 - accuracy: 0.6115 - val_loss: 0.7738 - val_accuracy: 0.5625\n",
      "Epoch 25/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7939 - accuracy: 0.5938\n",
      "Epoch 00025: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 960us/sample - loss: 0.7793 - accuracy: 0.6231 - val_loss: 0.7797 - val_accuracy: 0.5536\n",
      "Epoch 26/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7509 - accuracy: 0.6354\n",
      "Epoch 00026: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 907us/sample - loss: 0.7533 - accuracy: 0.6462 - val_loss: 0.7693 - val_accuracy: 0.5625\n",
      "Epoch 27/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8715 - accuracy: 0.5833\n",
      "Epoch 00027: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 917us/sample - loss: 0.8243 - accuracy: 0.6154 - val_loss: 0.7609 - val_accuracy: 0.5536\n",
      "Epoch 28/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7137 - accuracy: 0.6250\n",
      "Epoch 00028: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 910us/sample - loss: 0.7256 - accuracy: 0.6231 - val_loss: 0.7600 - val_accuracy: 0.5446\n",
      "Epoch 29/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.8548 - accuracy: 0.6198\n",
      "Epoch 00029: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 891us/sample - loss: 0.8511 - accuracy: 0.6154 - val_loss: 0.7672 - val_accuracy: 0.5625\n",
      "Epoch 30/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7830 - accuracy: 0.6250\n",
      "Epoch 00030: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 934us/sample - loss: 0.7807 - accuracy: 0.6269 - val_loss: 0.7985 - val_accuracy: 0.5804\n",
      "Epoch 31/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.6698 - accuracy: 0.6406\n",
      "Epoch 00031: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 922us/sample - loss: 0.7280 - accuracy: 0.6154 - val_loss: 0.8316 - val_accuracy: 0.6071\n",
      "Epoch 32/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7059 - accuracy: 0.6406\n",
      "Epoch 00032: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 922us/sample - loss: 0.7391 - accuracy: 0.6115 - val_loss: 0.8255 - val_accuracy: 0.6161\n",
      "Epoch 33/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7701 - accuracy: 0.6042\n",
      "Epoch 00033: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 921us/sample - loss: 0.7321 - accuracy: 0.6231 - val_loss: 0.8167 - val_accuracy: 0.6161\n",
      "Epoch 34/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7349 - accuracy: 0.6406\n",
      "Epoch 00034: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 916us/sample - loss: 0.7372 - accuracy: 0.6654 - val_loss: 0.7761 - val_accuracy: 0.6071\n",
      "Epoch 35/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7197 - accuracy: 0.6510\n",
      "Epoch 00035: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 899us/sample - loss: 0.7118 - accuracy: 0.6692 - val_loss: 0.7485 - val_accuracy: 0.6071\n",
      "Epoch 36/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7345 - accuracy: 0.6562\n",
      "Epoch 00036: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 954us/sample - loss: 0.7710 - accuracy: 0.6231 - val_loss: 0.7442 - val_accuracy: 0.5893\n",
      "Epoch 37/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7282 - accuracy: 0.6354\n",
      "Epoch 00037: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 915us/sample - loss: 0.7541 - accuracy: 0.6346 - val_loss: 0.7491 - val_accuracy: 0.5893\n",
      "Epoch 38/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.7590 - accuracy: 0.5898\n",
      "Epoch 00038: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 1ms/sample - loss: 0.7619 - accuracy: 0.5885 - val_loss: 0.7615 - val_accuracy: 0.5982\n",
      "Epoch 39/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.7596 - accuracy: 0.6289\n",
      "Epoch 00039: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 1ms/sample - loss: 0.7631 - accuracy: 0.6231 - val_loss: 0.7799 - val_accuracy: 0.5982\n",
      "Epoch 40/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.7438 - accuracy: 0.6250\n",
      "Epoch 00040: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 1ms/sample - loss: 0.7397 - accuracy: 0.6269 - val_loss: 0.7810 - val_accuracy: 0.6071\n",
      "Epoch 41/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.7593 - accuracy: 0.6289\n",
      "Epoch 00041: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 946us/sample - loss: 0.7565 - accuracy: 0.6308 - val_loss: 0.7733 - val_accuracy: 0.6071\n",
      "Epoch 42/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7438 - accuracy: 0.6667\n",
      "Epoch 00042: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 901us/sample - loss: 0.7195 - accuracy: 0.6654 - val_loss: 0.7705 - val_accuracy: 0.6250\n",
      "Epoch 43/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7254 - accuracy: 0.6719\n",
      "Epoch 00043: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 948us/sample - loss: 0.7282 - accuracy: 0.6538 - val_loss: 0.7690 - val_accuracy: 0.6250\n",
      "Epoch 44/50\n",
      "256/260 [============================>.] - ETA: 0s - loss: 0.7615 - accuracy: 0.6250\n",
      "Epoch 00044: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 1ms/sample - loss: 0.7545 - accuracy: 0.6308 - val_loss: 0.7554 - val_accuracy: 0.6250\n",
      "Epoch 45/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7467 - accuracy: 0.6198\n",
      "Epoch 00045: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 928us/sample - loss: 0.7597 - accuracy: 0.6038 - val_loss: 0.7491 - val_accuracy: 0.6250\n",
      "Epoch 46/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7451 - accuracy: 0.6302\n",
      "Epoch 00046: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 923us/sample - loss: 0.7706 - accuracy: 0.6231 - val_loss: 0.7558 - val_accuracy: 0.6250\n",
      "Epoch 47/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.6142 - accuracy: 0.7031\n",
      "Epoch 00047: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 935us/sample - loss: 0.6273 - accuracy: 0.6923 - val_loss: 0.7345 - val_accuracy: 0.6250\n",
      "Epoch 48/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.6990 - accuracy: 0.6458\n",
      "Epoch 00048: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 957us/sample - loss: 0.7172 - accuracy: 0.6423 - val_loss: 0.7268 - val_accuracy: 0.6339\n",
      "Epoch 49/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7352 - accuracy: 0.6146\n",
      "Epoch 00049: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 917us/sample - loss: 0.7411 - accuracy: 0.6231 - val_loss: 0.7473 - val_accuracy: 0.6339\n",
      "Epoch 50/50\n",
      "192/260 [=====================>........] - ETA: 0s - loss: 0.7351 - accuracy: 0.6250\n",
      "Epoch 00050: val_accuracy did not improve from 0.65179\n",
      "260/260 [==============================] - 0s 929us/sample - loss: 0.6874 - accuracy: 0.6615 - val_loss: 0.7777 - val_accuracy: 0.6429\n",
      "accuracy: 54.84%\n",
      "61.72% (+/- 4.98%)\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "\n",
    "\n",
    "seed = 7\n",
    "sample_shape=(32,32,32,1)\n",
    "np.random.seed(seed)\n",
    "# load pima indians dataset\n",
    "\n",
    "file_path='Kfoldmixseg-epo-{epoch:02d}-acc-{accuracy:.2f}-val-{val_accuracy:.2f}.hdf5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=file_path, monitor='val_accuracy', verbose=1, save_best_only=True,mode='auto')\n",
    "tensorboard =TensorBoard(log_dir=\"seq/log\")\n",
    "\n",
    "callback_lists=[tensorboard,checkpoint]\n",
    "y_all_orig=y_all[:,[0]]\n",
    "#K折交叉验证\n",
    "# define 10-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "cvscores = []\n",
    "for train, test in kfold.split(x_all2, y_all_orig):\n",
    "  # create model\n",
    "    \n",
    "    model=Sequential()\n",
    "\n",
    "#x_train=np.zeros(297,32,32,32,1)\n",
    "#y_train=np.zeros(297,2)\n",
    "#seg_train=np.zeros(297,32,32,32,1)\n",
    "    model.add(Conv3D(32, kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=sample_shape))\n",
    "    model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "    model.add(BatchNormalization(center=True, scale=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Conv3D(64, kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "    model.add(BatchNormalization(center=True, scale=True))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "\n",
    "    model.add(Conv3D(128, kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "    model.add(BatchNormalization(center=True, scale=True))\n",
    "    model.add(Dropout(0.6))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
    "#model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "   \n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              #loss='sparse_categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n",
    "              metrics=['accuracy'])\n",
    "     \n",
    "    # Fit the model\n",
    "   # y_train=to_categorical(y_train,2)\n",
    "    history = model.fit(x_all2[train], y_all[train],\n",
    "            batch_size=64,\n",
    "            epochs=50,\n",
    "            verbose=1,\n",
    "            shuffle=True,\n",
    "            validation_split=0.3,\n",
    "            callbacks=callback_lists)\n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(x_all2[test], y_all[test], verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#留一法，太慢,3层卷积，2个dense,后softmax\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "seed = 7\n",
    "sample_shape=(32,32,32,1)\n",
    "np.random.seed(seed)\n",
    "# load pima indians dataset\n",
    "\n",
    "file_path='Kfoldmixseg-epo-{epoch:02d}-acc-{accuracy:.2f}-val-{val_accuracy:.2f}.hdf5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=file_path, monitor='val_accuracy', verbose=1, save_best_only=True,mode='auto')\n",
    "tensorboard =TensorBoard(log_dir=\"seq/log\")\n",
    "best_keeper = ModelCheckpoint(filepath='best.h5', verbose=1, save_weights_only=False,\n",
    "                                  monitor='val_clf_acc', save_best_only=True, period=1, mode='max')\n",
    "csv_logger = CSVLogger('training.csv')\n",
    "early_stopping = EarlyStopping(monitor='val_clf_acc', min_delta=0, mode='max',\n",
    "                                   patience=30, verbose=1)\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.334, patience=10,\n",
    "                                   verbose=1, mode='min', epsilon=1.e-5, cooldown=2, min_lr=0)\n",
    "callback_lists=[tensorboard,checkpoint,early_stopping]\n",
    "# define 10-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "cvscores = []\n",
    "for train, test in loo.split(x_all2, y_all[:,[1]]):\n",
    "  # create model\n",
    "    \n",
    "    model=Sequential()\n",
    "\n",
    "#x_train=np.zeros(297,32,32,32,1)\n",
    "#y_train=np.zeros(297,2)\n",
    "#seg_train=np.zeros(297,32,32,32,1)\n",
    "    model.add(Conv3D(32, kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=sample_shape))\n",
    "    model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "    model.add(BatchNormalization(center=True, scale=True))\n",
    "    model.add(Dropout(0.8))\n",
    "    model.add(Conv3D(64, kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "    model.add(BatchNormalization(center=True, scale=True))\n",
    "    model.add(Dropout(0.7))\n",
    "\n",
    "\n",
    "    model.add(Conv3D(128, kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "    model.add(BatchNormalization(center=True, scale=True))\n",
    "    model.add(Dropout(0.6))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
    "#model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "   \n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              #loss='sparse_categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "     \n",
    "    # Fit the model\n",
    "   # y_train=to_categorical(y_train,2)\n",
    "    history = model.fit(x_all2[train], y_all[train],\n",
    "            batch_size=64,\n",
    "            epochs=20,\n",
    "            verbose=1,\n",
    "            shuffle=True,\n",
    "            validation_split=0.3,\n",
    "            callbacks=callback_lists)\n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(x_all2[test], y_all[test], verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path= \"seq2-seg-epo-{epoch:02d}-acc-{accuracy:.2f}-val-{val_accuracy:.2f}.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=file_path, monitor='val_accuracy', verbose=1, save_best_only=True,mode='auto')\n",
    "tensorboard =TensorBoard(log_dir=\"seq/log\")\n",
    "callback_lists=[tensorboard,checkpoint]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "print(\"Saving model to disk \\n\")\n",
    "mp = 'seqchaoduodai.h5'\n",
    "model.save(mp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_14 (Conv3D)           (None, 30, 30, 30, 32)    896       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_14 (MaxPooling (None, 15, 15, 15, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 15, 15, 15, 32)    128       \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 15, 15, 15, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_15 (Conv3D)           (None, 13, 13, 13, 64)    55360     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_15 (MaxPooling (None, 6, 6, 6, 64)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 6, 6, 6, 64)       256       \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 6, 6, 6, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv3d_16 (Conv3D)           (None, 4, 4, 4, 128)      221312    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_16 (MaxPooling (None, 2, 2, 2, 128)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 2, 2, 2, 128)      512       \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 2, 2, 2, 128)      0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 607,170\n",
      "Trainable params: 606,722\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sample_shape=(32,32,32,1)\n",
    "model=Sequential()\n",
    "#x_train=np.zeros(297,32,32,32,1)\n",
    "#y_train=np.zeros(297,2)\n",
    "#seg_train=np.zeros(297,32,32,32,1)\n",
    "model.add(Conv3D(32, kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=sample_shape))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(BatchNormalization(center=True, scale=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv3D(64, kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(BatchNormalization(center=True, scale=True))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "\n",
    "model.add(Conv3D(128, kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(BatchNormalization(center=True, scale=True))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              #loss='sparse_categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.SGD(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "# Fit data to model\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "    Learning rate is scheduled to be reduced after 20, 30 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "\n",
    "    if epoch >= 30:\n",
    "        lr *= 1e-2\n",
    "    elif epoch >= 20:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr\n",
    "\n",
    "\n",
    "def wd_schedule(epoch):\n",
    "    \"\"\"Weight Decay Schedule\n",
    "    Weight decay is scheduled to be reduced after 20, 30 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "    # Returns\n",
    "        wd (float32): weight decay\n",
    "    \"\"\"\n",
    "    wd = 1e-4\n",
    "\n",
    "    if epoch >= 30:\n",
    "        wd *= 1e-2\n",
    "    elif epoch >= 20:\n",
    "        wd *= 1e-1\n",
    "    print('Weight decay: ', wd)\n",
    "    return wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_4 (Conv3D)            (None, 30, 30, 30, 32)    896       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 15, 15, 15, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 15, 15, 15, 32)    128       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 15, 15, 15, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_5 (Conv3D)            (None, 13, 13, 13, 64)    55360     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3 (None, 7, 7, 7, 64)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 7, 7, 7, 64)       256       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 7, 7, 7, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv3d_6 (Conv3D)            (None, 5, 5, 5, 128)      221312    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_6 (MaxPooling3 (None, 3, 3, 3, 128)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 3, 3, 3, 128)      512       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 3, 3, 3, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv3d_7 (Conv3D)            (None, 1, 1, 1, 256)      884992    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_7 (MaxPooling3 (None, 1, 1, 1, 256)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 1, 1, 1, 256)      1024      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1, 1, 1, 256)      0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 1,296,578\n",
      "Trainable params: 1,295,618\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#加层后的seq_model\n",
    "sample_shape=(32,32,32,1)\n",
    "model=Sequential()\n",
    "\n",
    "#x_train=np.zeros(297,32,32,32,1)\n",
    "#y_train=np.zeros(297,2)\n",
    "#seg_train=np.zeros(297,32,32,32,1)\n",
    "model.add(Conv3D(32, kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=sample_shape))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2 ,2),padding='same'))\n",
    "model.add(BatchNormalization(center=True, scale=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv3D(64, kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2 ,2),padding='same'))\n",
    "model.add(BatchNormalization(center=True, scale=True))\n",
    "model.add(Dropout(0.6))\n",
    "\n",
    "\n",
    "model.add(Conv3D(128, kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2 ,2),padding='same'))\n",
    "model.add(BatchNormalization(center=True, scale=True))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv3D(256, kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2 ,2),padding='same'))\n",
    "model.add(BatchNormalization(center=True, scale=True))\n",
    "model.add(Dropout(0.6))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              #loss='sparse_categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_CLASSES = 2\n",
    "NUM_EPOCHS = 30\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "history = Model2.fit(x_train, y_train,\n",
    "            batch_size=16,\n",
    "            epochs=20,\n",
    "            verbose=1,\n",
    "            shuffle=True,\n",
    "            validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path= \"seq-mixdrps-seg-epo-{epoch:02d}-acc-{accuracy:.2f}-val-{val_accuracy:.2f}.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=file_path, monitor='val_accuracy', verbose=1, save_best_only=True,mode='auto')\n",
    "tensorboard =TensorBoard(log_dir=\"seq/log\")\n",
    "callback_lists=[tensorboard,checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history = model.fit(arg_x1,arg_y1,\n",
    "            batch_size=64,\n",
    "            epochs=100,\n",
    "            verbose=1,\n",
    "            shuffle=True,\n",
    "            validation_data=(xseg_train, yseg_train),\n",
    "            callbacks=callback_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 465 samples, validate on 465 samples\n",
      "Epoch 1/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.9743 - accuracy: 0.5446\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.53333, saving model to seq-mixdrps-seg-epo-01-acc-0.55-val-0.53.hdf5\n",
      "465/465 [==============================] - 2s 3ms/sample - loss: 0.9744 - accuracy: 0.5462 - val_loss: 5.7915 - val_accuracy: 0.5333\n",
      "Epoch 2/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.9795 - accuracy: 0.5469\n",
      "Epoch 00002: val_accuracy improved from 0.53333 to 0.53763, saving model to seq-mixdrps-seg-epo-02-acc-0.54-val-0.54.hdf5\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.9823 - accuracy: 0.5441 - val_loss: 3.1045 - val_accuracy: 0.5376\n",
      "Epoch 3/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.9180 - accuracy: 0.5558\n",
      "Epoch 00003: val_accuracy improved from 0.53763 to 0.54409, saving model to seq-mixdrps-seg-epo-03-acc-0.55-val-0.54.hdf5\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.9175 - accuracy: 0.5527 - val_loss: 1.5785 - val_accuracy: 0.5441\n",
      "Epoch 4/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.9955 - accuracy: 0.4911\n",
      "Epoch 00004: val_accuracy improved from 0.54409 to 0.54839, saving model to seq-mixdrps-seg-epo-04-acc-0.50-val-0.55.hdf5\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.9935 - accuracy: 0.4968 - val_loss: 1.2137 - val_accuracy: 0.5484\n",
      "Epoch 5/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.9297 - accuracy: 0.5580\n",
      "Epoch 00005: val_accuracy improved from 0.54839 to 0.55054, saving model to seq-mixdrps-seg-epo-05-acc-0.56-val-0.55.hdf5\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.9260 - accuracy: 0.5613 - val_loss: 1.0872 - val_accuracy: 0.5505\n",
      "Epoch 6/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.9024 - accuracy: 0.5491\n",
      "Epoch 00006: val_accuracy improved from 0.55054 to 0.57204, saving model to seq-mixdrps-seg-epo-06-acc-0.55-val-0.57.hdf5\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.9045 - accuracy: 0.5462 - val_loss: 0.8560 - val_accuracy: 0.5720\n",
      "Epoch 7/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.9741 - accuracy: 0.4933\n",
      "Epoch 00007: val_accuracy did not improve from 0.57204\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.9721 - accuracy: 0.4946 - val_loss: 0.8248 - val_accuracy: 0.5570\n",
      "Epoch 8/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.9543 - accuracy: 0.5179\n",
      "Epoch 00008: val_accuracy improved from 0.57204 to 0.58280, saving model to seq-mixdrps-seg-epo-08-acc-0.52-val-0.58.hdf5\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.9515 - accuracy: 0.5183 - val_loss: 0.7668 - val_accuracy: 0.5828\n",
      "Epoch 9/100\n",
      "384/465 [=======================>......] - ETA: 0s - loss: 0.9144 - accuracy: 0.5078\n",
      "Epoch 00009: val_accuracy improved from 0.58280 to 0.59570, saving model to seq-mixdrps-seg-epo-09-acc-0.50-val-0.60.hdf5\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.9111 - accuracy: 0.5011 - val_loss: 0.7487 - val_accuracy: 0.5957\n",
      "Epoch 10/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.8848 - accuracy: 0.5357\n",
      "Epoch 00010: val_accuracy did not improve from 0.59570\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.8958 - accuracy: 0.5312 - val_loss: 0.7100 - val_accuracy: 0.5957\n",
      "Epoch 11/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.8758 - accuracy: 0.5357\n",
      "Epoch 00011: val_accuracy improved from 0.59570 to 0.61720, saving model to seq-mixdrps-seg-epo-11-acc-0.54-val-0.62.hdf5\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.8616 - accuracy: 0.5441 - val_loss: 0.6977 - val_accuracy: 0.6172\n",
      "Epoch 12/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.8589 - accuracy: 0.5625\n",
      "Epoch 00012: val_accuracy improved from 0.61720 to 0.63011, saving model to seq-mixdrps-seg-epo-12-acc-0.56-val-0.63.hdf5\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.8597 - accuracy: 0.5591 - val_loss: 0.6930 - val_accuracy: 0.6301\n",
      "Epoch 13/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.8461 - accuracy: 0.5759\n",
      "Epoch 00013: val_accuracy did not improve from 0.63011\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.8462 - accuracy: 0.5720 - val_loss: 0.6837 - val_accuracy: 0.6280\n",
      "Epoch 14/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.8597 - accuracy: 0.5826\n",
      "Epoch 00014: val_accuracy did not improve from 0.63011\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.8600 - accuracy: 0.5785 - val_loss: 0.6788 - val_accuracy: 0.6129\n",
      "Epoch 15/100\n",
      "384/465 [=======================>......] - ETA: 0s - loss: 0.9076 - accuracy: 0.5234\n",
      "Epoch 00015: val_accuracy did not improve from 0.63011\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.8944 - accuracy: 0.5355 - val_loss: 0.6762 - val_accuracy: 0.6215\n",
      "Epoch 16/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.8575 - accuracy: 0.5759\n",
      "Epoch 00016: val_accuracy did not improve from 0.63011\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.8465 - accuracy: 0.5828 - val_loss: 0.6769 - val_accuracy: 0.6301\n",
      "Epoch 17/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.8409 - accuracy: 0.5513\n",
      "Epoch 00017: val_accuracy improved from 0.63011 to 0.64516, saving model to seq-mixdrps-seg-epo-17-acc-0.55-val-0.65.hdf5\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.8441 - accuracy: 0.5462 - val_loss: 0.6729 - val_accuracy: 0.6452\n",
      "Epoch 18/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.8656 - accuracy: 0.5379\n",
      "Epoch 00018: val_accuracy improved from 0.64516 to 0.64731, saving model to seq-mixdrps-seg-epo-18-acc-0.54-val-0.65.hdf5\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.8623 - accuracy: 0.5398 - val_loss: 0.6767 - val_accuracy: 0.6473\n",
      "Epoch 19/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.9009 - accuracy: 0.5446\n",
      "Epoch 00019: val_accuracy improved from 0.64731 to 0.65161, saving model to seq-mixdrps-seg-epo-19-acc-0.55-val-0.65.hdf5\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.8942 - accuracy: 0.5505 - val_loss: 0.6803 - val_accuracy: 0.6516\n",
      "Epoch 20/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.8096 - accuracy: 0.6004\n",
      "Epoch 00020: val_accuracy improved from 0.65161 to 0.65376, saving model to seq-mixdrps-seg-epo-20-acc-0.60-val-0.65.hdf5\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.8103 - accuracy: 0.6000 - val_loss: 0.6777 - val_accuracy: 0.6538\n",
      "Epoch 21/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7785 - accuracy: 0.6116\n",
      "Epoch 00021: val_accuracy improved from 0.65376 to 0.65591, saving model to seq-mixdrps-seg-epo-21-acc-0.61-val-0.66.hdf5\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.7805 - accuracy: 0.6108 - val_loss: 0.6772 - val_accuracy: 0.6559\n",
      "Epoch 22/100\n",
      "384/465 [=======================>......] - ETA: 0s - loss: 0.8479 - accuracy: 0.5625\n",
      "Epoch 00022: val_accuracy did not improve from 0.65591\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.8371 - accuracy: 0.5677 - val_loss: 0.6689 - val_accuracy: 0.6473\n",
      "Epoch 23/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7967 - accuracy: 0.5938\n",
      "Epoch 00023: val_accuracy did not improve from 0.65591\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.8027 - accuracy: 0.5871 - val_loss: 0.6798 - val_accuracy: 0.6452\n",
      "Epoch 24/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.8394 - accuracy: 0.5737\n",
      "Epoch 00024: val_accuracy did not improve from 0.65591\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.8365 - accuracy: 0.5785 - val_loss: 0.6787 - val_accuracy: 0.6495\n",
      "Epoch 25/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7873 - accuracy: 0.5692\n",
      "Epoch 00025: val_accuracy did not improve from 0.65591\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7882 - accuracy: 0.5699 - val_loss: 0.6827 - val_accuracy: 0.6495\n",
      "Epoch 26/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.8142 - accuracy: 0.5580\n",
      "Epoch 00026: val_accuracy did not improve from 0.65591\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.8208 - accuracy: 0.5527 - val_loss: 0.6860 - val_accuracy: 0.6495\n",
      "Epoch 27/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7506 - accuracy: 0.5960\n",
      "Epoch 00027: val_accuracy did not improve from 0.65591\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7438 - accuracy: 0.5978 - val_loss: 0.6798 - val_accuracy: 0.6516\n",
      "Epoch 28/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7261 - accuracy: 0.6205\n",
      "Epoch 00028: val_accuracy improved from 0.65591 to 0.65806, saving model to seq-mixdrps-seg-epo-28-acc-0.62-val-0.66.hdf5\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.7325 - accuracy: 0.6151 - val_loss: 0.6708 - val_accuracy: 0.6581\n",
      "Epoch 29/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.8227 - accuracy: 0.5536\n",
      "Epoch 00029: val_accuracy did not improve from 0.65806\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.8184 - accuracy: 0.5548 - val_loss: 0.6741 - val_accuracy: 0.6516\n",
      "Epoch 30/100\n",
      "384/465 [=======================>......] - ETA: 0s - loss: 0.8003 - accuracy: 0.5625\n",
      "Epoch 00030: val_accuracy did not improve from 0.65806\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.8090 - accuracy: 0.5656 - val_loss: 0.6683 - val_accuracy: 0.6581\n",
      "Epoch 31/100\n",
      "384/465 [=======================>......] - ETA: 0s - loss: 0.8571 - accuracy: 0.5469\n",
      "Epoch 00031: val_accuracy did not improve from 0.65806\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.8533 - accuracy: 0.5505 - val_loss: 0.6683 - val_accuracy: 0.6559\n",
      "Epoch 32/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7855 - accuracy: 0.5826\n",
      "Epoch 00032: val_accuracy did not improve from 0.65806\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7812 - accuracy: 0.5828 - val_loss: 0.6817 - val_accuracy: 0.6559\n",
      "Epoch 33/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.8163 - accuracy: 0.5938\n",
      "Epoch 00033: val_accuracy did not improve from 0.65806\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.8054 - accuracy: 0.6000 - val_loss: 0.6878 - val_accuracy: 0.6581\n",
      "Epoch 34/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7682 - accuracy: 0.5960\n",
      "Epoch 00034: val_accuracy did not improve from 0.65806\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.7719 - accuracy: 0.5914 - val_loss: 0.6891 - val_accuracy: 0.6538\n",
      "Epoch 35/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.8111 - accuracy: 0.6004\n",
      "Epoch 00035: val_accuracy did not improve from 0.65806\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.8027 - accuracy: 0.6065 - val_loss: 0.6829 - val_accuracy: 0.6581\n",
      "Epoch 36/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.8252 - accuracy: 0.6049\n",
      "Epoch 00036: val_accuracy did not improve from 0.65806\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.8209 - accuracy: 0.6043 - val_loss: 0.6888 - val_accuracy: 0.6473\n",
      "Epoch 37/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.8113 - accuracy: 0.5737\n",
      "Epoch 00037: val_accuracy did not improve from 0.65806\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.8290 - accuracy: 0.5656 - val_loss: 0.6875 - val_accuracy: 0.6473\n",
      "Epoch 38/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.8280 - accuracy: 0.5714\n",
      "Epoch 00038: val_accuracy improved from 0.65806 to 0.66022, saving model to seq-mixdrps-seg-epo-38-acc-0.57-val-0.66.hdf5\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.8293 - accuracy: 0.5699 - val_loss: 0.6719 - val_accuracy: 0.6602\n",
      "Epoch 39/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.8068 - accuracy: 0.5893\n",
      "Epoch 00039: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.8116 - accuracy: 0.5892 - val_loss: 0.6752 - val_accuracy: 0.6602\n",
      "Epoch 40/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7970 - accuracy: 0.5670\n",
      "Epoch 00040: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7989 - accuracy: 0.5720 - val_loss: 0.6814 - val_accuracy: 0.6581\n",
      "Epoch 41/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7626 - accuracy: 0.5826\n",
      "Epoch 00041: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7673 - accuracy: 0.5828 - val_loss: 0.6890 - val_accuracy: 0.6495\n",
      "Epoch 42/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.8253 - accuracy: 0.5625\n",
      "Epoch 00042: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.8224 - accuracy: 0.5613 - val_loss: 0.6831 - val_accuracy: 0.6581\n",
      "Epoch 43/100\n",
      "384/465 [=======================>......] - ETA: 0s - loss: 0.7340 - accuracy: 0.6172\n",
      "Epoch 00043: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7504 - accuracy: 0.6086 - val_loss: 0.6854 - val_accuracy: 0.6602\n",
      "Epoch 44/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7252 - accuracy: 0.6161\n",
      "Epoch 00044: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7240 - accuracy: 0.6172 - val_loss: 0.6874 - val_accuracy: 0.6581\n",
      "Epoch 45/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7496 - accuracy: 0.6094\n",
      "Epoch 00045: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7417 - accuracy: 0.6151 - val_loss: 0.7017 - val_accuracy: 0.6495\n",
      "Epoch 46/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.8220 - accuracy: 0.5759\n",
      "Epoch 00046: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.8266 - accuracy: 0.5763 - val_loss: 0.6978 - val_accuracy: 0.6430\n",
      "Epoch 47/100\n",
      "384/465 [=======================>......] - ETA: 0s - loss: 0.7895 - accuracy: 0.5599\n",
      "Epoch 00047: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7883 - accuracy: 0.5570 - val_loss: 0.7128 - val_accuracy: 0.6495\n",
      "Epoch 48/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7472 - accuracy: 0.6071\n",
      "Epoch 00048: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7420 - accuracy: 0.6108 - val_loss: 0.7012 - val_accuracy: 0.6452\n",
      "Epoch 49/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7166 - accuracy: 0.6116\n",
      "Epoch 00049: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7166 - accuracy: 0.6022 - val_loss: 0.6951 - val_accuracy: 0.6430\n",
      "Epoch 50/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7460 - accuracy: 0.5915\n",
      "Epoch 00050: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7524 - accuracy: 0.5935 - val_loss: 0.6897 - val_accuracy: 0.6430\n",
      "Epoch 51/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.6931 - accuracy: 0.6161\n",
      "Epoch 00051: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7048 - accuracy: 0.6086 - val_loss: 0.6966 - val_accuracy: 0.6452\n",
      "Epoch 52/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7560 - accuracy: 0.5871\n",
      "Epoch 00052: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7543 - accuracy: 0.5849 - val_loss: 0.6962 - val_accuracy: 0.6452\n",
      "Epoch 53/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7374 - accuracy: 0.6116\n",
      "Epoch 00053: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7537 - accuracy: 0.6086 - val_loss: 0.7034 - val_accuracy: 0.6473\n",
      "Epoch 54/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7431 - accuracy: 0.5960\n",
      "Epoch 00054: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7439 - accuracy: 0.5957 - val_loss: 0.6948 - val_accuracy: 0.6473\n",
      "Epoch 55/100\n",
      "384/465 [=======================>......] - ETA: 0s - loss: 0.8066 - accuracy: 0.6042\n",
      "Epoch 00055: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.8004 - accuracy: 0.5849 - val_loss: 0.6896 - val_accuracy: 0.6452\n",
      "Epoch 56/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7277 - accuracy: 0.6049\n",
      "Epoch 00056: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.7203 - accuracy: 0.6108 - val_loss: 0.6990 - val_accuracy: 0.6430\n",
      "Epoch 57/100\n",
      "384/465 [=======================>......] - ETA: 0s - loss: 0.7214 - accuracy: 0.5990\n",
      "Epoch 00057: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7508 - accuracy: 0.5892 - val_loss: 0.7056 - val_accuracy: 0.6473\n",
      "Epoch 58/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7899 - accuracy: 0.5558\n",
      "Epoch 00058: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.7903 - accuracy: 0.5548 - val_loss: 0.7066 - val_accuracy: 0.6452\n",
      "Epoch 59/100\n",
      "384/465 [=======================>......] - ETA: 0s - loss: 0.7508 - accuracy: 0.5990\n",
      "Epoch 00059: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.7655 - accuracy: 0.5914 - val_loss: 0.7075 - val_accuracy: 0.6430\n",
      "Epoch 60/100\n",
      "384/465 [=======================>......] - ETA: 0s - loss: 0.7875 - accuracy: 0.5938\n",
      "Epoch 00060: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.7659 - accuracy: 0.5914 - val_loss: 0.7023 - val_accuracy: 0.6495\n",
      "Epoch 61/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7705 - accuracy: 0.5938\n",
      "Epoch 00061: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7785 - accuracy: 0.5892 - val_loss: 0.7270 - val_accuracy: 0.6323\n",
      "Epoch 62/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7304 - accuracy: 0.6027\n",
      "Epoch 00062: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7293 - accuracy: 0.6108 - val_loss: 0.7358 - val_accuracy: 0.6366\n",
      "Epoch 63/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7215 - accuracy: 0.6295\n",
      "Epoch 00063: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7269 - accuracy: 0.6237 - val_loss: 0.7397 - val_accuracy: 0.6366\n",
      "Epoch 64/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7655 - accuracy: 0.5625\n",
      "Epoch 00064: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7701 - accuracy: 0.5634 - val_loss: 0.7271 - val_accuracy: 0.6323\n",
      "Epoch 65/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7890 - accuracy: 0.5982\n",
      "Epoch 00065: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7874 - accuracy: 0.5935 - val_loss: 0.7250 - val_accuracy: 0.6387\n",
      "Epoch 66/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.6855 - accuracy: 0.6451\n",
      "Epoch 00066: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.6822 - accuracy: 0.6452 - val_loss: 0.7373 - val_accuracy: 0.6344\n",
      "Epoch 67/100\n",
      "384/465 [=======================>......] - ETA: 0s - loss: 0.7677 - accuracy: 0.5833\n",
      "Epoch 00067: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.7711 - accuracy: 0.5806 - val_loss: 0.7492 - val_accuracy: 0.6366\n",
      "Epoch 68/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7046 - accuracy: 0.6518\n",
      "Epoch 00068: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.6949 - accuracy: 0.6559 - val_loss: 0.7427 - val_accuracy: 0.6344\n",
      "Epoch 69/100\n",
      "384/465 [=======================>......] - ETA: 0s - loss: 0.7201 - accuracy: 0.6250\n",
      "Epoch 00069: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7221 - accuracy: 0.6172 - val_loss: 0.7121 - val_accuracy: 0.6430\n",
      "Epoch 70/100\n",
      "384/465 [=======================>......] - ETA: 0s - loss: 0.6983 - accuracy: 0.6224\n",
      "Epoch 00070: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7133 - accuracy: 0.6043 - val_loss: 0.7094 - val_accuracy: 0.6473\n",
      "Epoch 71/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7595 - accuracy: 0.5804\n",
      "Epoch 00071: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.7587 - accuracy: 0.5849 - val_loss: 0.7186 - val_accuracy: 0.6473\n",
      "Epoch 72/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7562 - accuracy: 0.5871\n",
      "Epoch 00072: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7612 - accuracy: 0.5828 - val_loss: 0.7123 - val_accuracy: 0.6430\n",
      "Epoch 73/100\n",
      "384/465 [=======================>......] - ETA: 0s - loss: 0.7432 - accuracy: 0.6042\n",
      "Epoch 00073: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7575 - accuracy: 0.5935 - val_loss: 0.7164 - val_accuracy: 0.6409\n",
      "Epoch 74/100\n",
      "384/465 [=======================>......] - ETA: 0s - loss: 0.7203 - accuracy: 0.6458\n",
      "Epoch 00074: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7179 - accuracy: 0.6344 - val_loss: 0.7280 - val_accuracy: 0.6344\n",
      "Epoch 75/100\n",
      "384/465 [=======================>......] - ETA: 0s - loss: 0.7337 - accuracy: 0.5859\n",
      "Epoch 00075: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7279 - accuracy: 0.5785 - val_loss: 0.7452 - val_accuracy: 0.6344\n",
      "Epoch 76/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7297 - accuracy: 0.6027\n",
      "Epoch 00076: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7313 - accuracy: 0.6043 - val_loss: 0.7451 - val_accuracy: 0.6323\n",
      "Epoch 77/100\n",
      "384/465 [=======================>......] - ETA: 0s - loss: 0.7938 - accuracy: 0.5495\n",
      "Epoch 00077: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7745 - accuracy: 0.5763 - val_loss: 0.7400 - val_accuracy: 0.6344\n",
      "Epoch 78/100\n",
      "384/465 [=======================>......] - ETA: 0s - loss: 0.7080 - accuracy: 0.6146\n",
      "Epoch 00078: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7328 - accuracy: 0.5957 - val_loss: 0.7331 - val_accuracy: 0.6344\n",
      "Epoch 79/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7429 - accuracy: 0.6094\n",
      "Epoch 00079: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7386 - accuracy: 0.6108 - val_loss: 0.7217 - val_accuracy: 0.6323\n",
      "Epoch 80/100\n",
      "384/465 [=======================>......] - ETA: 0s - loss: 0.7324 - accuracy: 0.5990\n",
      "Epoch 00080: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.7289 - accuracy: 0.6065 - val_loss: 0.7232 - val_accuracy: 0.6323\n",
      "Epoch 81/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7072 - accuracy: 0.5848\n",
      "Epoch 00081: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.7053 - accuracy: 0.5914 - val_loss: 0.7317 - val_accuracy: 0.6344\n",
      "Epoch 82/100\n",
      "384/465 [=======================>......] - ETA: 0s - loss: 0.7420 - accuracy: 0.6250\n",
      "Epoch 00082: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.7313 - accuracy: 0.6323 - val_loss: 0.7517 - val_accuracy: 0.6323\n",
      "Epoch 83/100\n",
      "384/465 [=======================>......] - ETA: 0s - loss: 0.6971 - accuracy: 0.6380\n",
      "Epoch 00083: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.7075 - accuracy: 0.6258 - val_loss: 0.7545 - val_accuracy: 0.6344\n",
      "Epoch 84/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.6570 - accuracy: 0.6339\n",
      "Epoch 00084: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.6516 - accuracy: 0.6387 - val_loss: 0.7511 - val_accuracy: 0.6323\n",
      "Epoch 85/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7334 - accuracy: 0.6094\n",
      "Epoch 00085: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.7272 - accuracy: 0.6129 - val_loss: 0.7419 - val_accuracy: 0.6366\n",
      "Epoch 86/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7334 - accuracy: 0.5781\n",
      "Epoch 00086: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7253 - accuracy: 0.5849 - val_loss: 0.7391 - val_accuracy: 0.6366\n",
      "Epoch 87/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.6873 - accuracy: 0.6272\n",
      "Epoch 00087: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.6855 - accuracy: 0.6323 - val_loss: 0.7392 - val_accuracy: 0.6344\n",
      "Epoch 88/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7579 - accuracy: 0.5915\n",
      "Epoch 00088: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.7601 - accuracy: 0.5849 - val_loss: 0.7285 - val_accuracy: 0.6344\n",
      "Epoch 89/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.6926 - accuracy: 0.6116\n",
      "Epoch 00089: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.6867 - accuracy: 0.6172 - val_loss: 0.7304 - val_accuracy: 0.6366\n",
      "Epoch 90/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.6937 - accuracy: 0.6183\n",
      "Epoch 00090: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.6891 - accuracy: 0.6215 - val_loss: 0.7498 - val_accuracy: 0.6387\n",
      "Epoch 91/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7281 - accuracy: 0.6138\n",
      "Epoch 00091: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7278 - accuracy: 0.6151 - val_loss: 0.7524 - val_accuracy: 0.6387\n",
      "Epoch 92/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.6507 - accuracy: 0.6696\n",
      "Epoch 00092: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.6441 - accuracy: 0.6753 - val_loss: 0.7438 - val_accuracy: 0.6409\n",
      "Epoch 93/100\n",
      "384/465 [=======================>......] - ETA: 0s - loss: 0.7166 - accuracy: 0.6094\n",
      "Epoch 00093: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7301 - accuracy: 0.6065 - val_loss: 0.7512 - val_accuracy: 0.6409\n",
      "Epoch 94/100\n",
      "384/465 [=======================>......] - ETA: 0s - loss: 0.7255 - accuracy: 0.5938\n",
      "Epoch 00094: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.7178 - accuracy: 0.6000 - val_loss: 0.7473 - val_accuracy: 0.6387\n",
      "Epoch 95/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7652 - accuracy: 0.5871\n",
      "Epoch 00095: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7637 - accuracy: 0.5871 - val_loss: 0.7292 - val_accuracy: 0.6387\n",
      "Epoch 96/100\n",
      "384/465 [=======================>......] - ETA: 0s - loss: 0.7308 - accuracy: 0.6198\n",
      "Epoch 00096: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7138 - accuracy: 0.6237 - val_loss: 0.7432 - val_accuracy: 0.6387\n",
      "Epoch 97/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7375 - accuracy: 0.6004\n",
      "Epoch 00097: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.7311 - accuracy: 0.6086 - val_loss: 0.7373 - val_accuracy: 0.6409\n",
      "Epoch 98/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.6720 - accuracy: 0.6295\n",
      "Epoch 00098: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 1s 1ms/sample - loss: 0.6714 - accuracy: 0.6301 - val_loss: 0.7306 - val_accuracy: 0.6430\n",
      "Epoch 99/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7641 - accuracy: 0.5781\n",
      "Epoch 00099: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7539 - accuracy: 0.5849 - val_loss: 0.7417 - val_accuracy: 0.6344\n",
      "Epoch 100/100\n",
      "448/465 [===========================>..] - ETA: 0s - loss: 0.7487 - accuracy: 0.6027\n",
      "Epoch 00100: val_accuracy did not improve from 0.66022\n",
      "465/465 [==============================] - 0s 1ms/sample - loss: 0.7533 - accuracy: 0.6000 - val_loss: 0.7621 - val_accuracy: 0.6151\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(x_all2,y_all,\n",
    "            batch_size=64,\n",
    "            epochs=100,\n",
    "            verbose=1,\n",
    "            shuffle=True,\n",
    "            validation_data=(x_all2,y_all),\n",
    "            callbacks=callback_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xseg_train.shape)\n",
    "x_val=x_all2[num+1:all_num-1]\n",
    "y_val=y_all[num+1:all_num-1]\n",
    "score_test=model.evaluate(x_val[0:17],y_val[0:17],verbose=1)\n",
    "print('Testing loss: %.4f, Testing accuracy: %.2f%%' % (score_test[0],score_test[1]*100))\n",
    "score_test=model.evaluate(arg_x1,arg_y1)\n",
    "print('Testing loss: %.4f, Testing accuracy: %.2f%%' % (score_test[0],score_test[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xseg_train.shape)\n",
    "print(x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "#file_path1='./seq_model/epo-06-acc-0.64-val-0.61.hdf5'\n",
    "#file_path1='./TrainedModel/log/SequentialModel1_6243.h5'\n",
    "file_path1='seq-arg-seg-epo-02-acc-0.59-val-0.65.hdf5'\n",
    "#file_path1='../input/train-data/seq-seg-base-epo-09-acc-0.55-val-0.65.hdf5'\n",
    "model= load_model(file_path1)\n",
    "#val_pre = model.predict(x_val, batch_size=5, verbose=1)\n",
    "#score=model.evaluate\n",
    "#test_pre=model.predict(x_test,batch_size=3,verbose=1)\n",
    "score_test=model.evaluate(x_all2,y_all,verbose=1)\n",
    "print('Testing loss: %.4f, Testing accuracy: %.2f%%' % (score_test[0],score_test[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#参考https://blog.csdn.net/acbattle/article/details/97824441\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import tensorflow as tf\n",
    "# AUC for a binary classifier\n",
    "def auc(y_true, y_pred):\n",
    "    ptas = tf.stack([binary_PTA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)\n",
    "    pfas = tf.stack([binary_PFA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)\n",
    "    pfas = tf.concat([tf.ones((1,)) ,pfas],axis=0)\n",
    "    binSizes = -(pfas[1:]-pfas[:-1])\n",
    "    s = ptas * binSizes\n",
    "    return K.sum(s, axis=0)\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# PFA, prob false alert for binary classifier\n",
    "def binary_PFA(y_true, y_pred, threshold=K.variable(value=0.5)):\n",
    "    y_pred = K.cast(y_pred >= threshold, 'float32')\n",
    "    # N = total number of negative labels\n",
    "    N = K.sum(1 - y_true)\n",
    "    # FP = total number of false alerts, alerts from the negative class labels\n",
    "    FP = K.sum(y_pred - y_pred * y_true)\n",
    "    return FP/N\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# P_TA prob true alerts for binary classifier\n",
    "def binary_PTA(y_true, y_pred, threshold=K.variable(value=0.5)):\n",
    "    y_pred = K.cast(y_pred >= threshold, 'float32')\n",
    "    # P = total number of positive labels\n",
    "    P = K.sum(y_true)\n",
    "    # TP = total number of correct alerts, alerts from the positive class labels\n",
    "    TP = K.sum(y_pred * y_true)\n",
    "    return TP/P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.66764295, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "ytrain_pre=model.predict(xseg_train)\n",
    "a=auc(yseg_train,ytrain_pre)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#634  667\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "test_info=pd.read_csv(\"../input/train-data/test_result.csv\")\n",
    "test_nodule_path=\"../input/train-data/test/test/\"\n",
    "name = test_info.loc[:, 'name']\n",
    "test_num=len(name)\n",
    "xs = np.empty((test_num,*(32,32,32), 1))\n",
    "\n",
    "for i in range(test_num):\n",
    "        with np.load(os.path.join(test_nodule_path, '%s.npz' % name[i])) as npz:\n",
    "            voxel=npz['voxel'][34:66,34:66,34:66]\n",
    "            voxel=np.expand_dims(voxel,axis=-1)\n",
    "            xs[i,]=voxel\n",
    "x_test=xs\n",
    "print(x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#预测并保存\n",
    "y_test_pre=model.predict(xseg_test)\n",
    "print(y_test_pre.shape)\n",
    "\n",
    "y_test1=y_test_pre[:,[1]]\n",
    "y_test1[5]\n",
    "y_test_pre[[5]]\n",
    "test_num=len(x_test)\n",
    "import pandas as pd\n",
    "\n",
    "file_name=\"submission seq-seg-acc0.73val0.73..csv\"\n",
    "test_list = pd.read_table(\"../input/train-data/test_result.csv\",sep = \",\")['name']\n",
    "test_name = np.array(test_list).reshape(test_num)\n",
    "print(y_test_pre.shape)\n",
    "load_predicted = np.array(y_test1).reshape(test_num)\n",
    "load_test_dict = {'name':test_name, 'Predicted':load_predicted}\n",
    "load_result = pd.DataFrame(load_test_dict, index = [0 for _ in range(test_num)])\n",
    "load_result.to_csv(file_name, index = False, sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "val_dataset=dataset1.ClfSegDataset(crop_size=32, subset=[1,2,3,0])\n",
    "val_num=len(val_dataset)\n",
    "x_val= np.ones((val_num,32,32,32,1))\n",
    "y_val= np.ones((val_num,1))\n",
    "print(x_val.shape)\n",
    "i=0\n",
    "while i<val_num:\n",
    "    x_val[i],(y_val[i],seg)=val_dataset[i]\n",
    "    i+=1\n",
    "\n",
    "y_val=to_categorical(y_val,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_test=model.evaluate(x_all,y_all,verbose=0)\n",
    "print('Testing loss: %.4f, Testing accuracy: %.2f%%' % (score_test[0],score_test[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(test_pre.shape)\n",
    "\n",
    "y_test1=test_pre[:,[1]]\n",
    "y_test1[5]\n",
    "print('load后预测',y_test_pre[[6]])\n",
    "y_test1=y_test_pre[:,[1]]\n",
    "y_test1[5]\n",
    "y_test_pre[[5]]\n",
    "print('load前预测',y_test_pre[[6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.title(\"Sequential Model\")\n",
    "plt.savefig(\"seq_model train val.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
